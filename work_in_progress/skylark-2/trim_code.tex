\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{mathtools} % defines pmatrix*
\usepackage{verbatim}
\usepackage{parskip}
\usepackage[margin=72pt]{geometry}
\usepackage{color}
\begin{document}
\let\Sum=\sum
\newcommand{\diag}[1]{\operatorname{diag}(#1)}
\newcommand{\var}[1]{\operatorname{var}(#1)}
\newcommand{\cov}[1]{\operatorname{cov}(#1)}
\newcommand{\se}[1]{\operatorname{S.E.}(#1)}
\newcommand{\Mu}{{\mu_{+}}}
\title{TRIMR --- TRIM in R}
\author{Patrick Bogaart, Mark van der Loo, Jeroen Pannekoek, Arco van Strien}
\date{\today}
\maketitle
\tableofcontents 



\section{Parameter estimation}\par

This Section describes the core TRIM function, which estimates the TRIM parameters.\par

First intriduce some helper functions.\par
\verb~> set_trim_verbose <- function(verbose=FALSE){~\newline
\verb~>   stopifnot(isTRUE(verbose)|!isTRUE(verbose))~\newline
\verb~>   options(trim_verbose=verbose)~\newline
\verb~> }~\newline
\verb~> set_trim_verbose(TRUE)~\par

Convenience function for console output during runs\par
\verb~> rprintf <- function(fmt,...) { if(getOption("trim_verbose")) cat(sprintf(fmt,...)) }~\par

Similar, but for object/summary printing\par
\verb~> printf <- function(fmt,...) {cat(sprintf(fmt,...))}~\par

Let's het started with the main workhorse function.\par

\verb~> trim_estimate <- function(count, time.id, site.id, covars=list(),~\newline
\verb~>             model=2, serialcor=FALSE, overdisp=FALSE,~\newline
\verb~>             changepoints=integer(0)) {~\par



\subsection{Preparation}\par

Check the arguments. \verb!count! should be a vector of numerics.\par
\verb~>   stopifnot(class(count) %in% c("integer","numeric"))~\newline
\verb~>   n = length(count)~\par

\verb!time.id! should be an ordered factor, or a vector of consecutive years or numbers
Note the use of "any" because of multiple classes for ordered factors\par
\verb~>   stopifnot(any(class(time.id) %in% c("integer","numeric","factor")))~\newline
\verb~>   if (any(class(time.id) %in% c("integer","numeric"))) {~\newline
\verb~>     check = unique(diff(sort(unique(time.id))))~\newline
\verb~>     stopifnot(check==1 && length(check)==1)~\newline
\verb~>   }~\newline
\verb~>   stopifnot(length(time.id)==n)~\par
Convert the time points to a factor\par

\verb!site.id! should be a vector of numbers, strings or factors\par
\verb~>   stopifnot(class(site.id) %in% c("integer","character","factor"))~\newline
\verb~>   stopifnot(length(site.id)==n)~\par

\verb!covars! should be a list where each element (if any) is a vector\par
\verb~>   stopifnot(class(covars)=="list")~\newline
\verb~>   ncovar = length(covars)~\newline
\verb~>   use.covars <- ncovar>0~\newline
\verb~>   if (use.covars) {~\newline
\verb~>     for (i in 1:ncovar) stopifnot(class(covars[[i]]) %in% c("integer","numeric"))~\par

Also, each covariate $i$ should be a number (ID) ranging $1\ldots nclass_i$\par
\verb~>     nclass <- numeric(ncovar)~\newline
\verb~>     for (i in 1:ncovar) {~\newline
\verb~>       cv <- covars[[i]]  ~{\sffamily\# The vector of covariate class ID's}\newline
\verb~>       stopifnot(min(cv)==1)  ~{\sffamily\# Assert lower end of range}\newline
\verb~>       nclass[i] = max(cv)  ~{\sffamily\# Upper end of range}\newline
\verb~>       stopifnot(nclass[i]>1)  ~{\sffamily\# Assert upper end}\newline
\verb~>       stopifnot(length(unique(cv))==nclass[i])  ~{\sffamily\# Assert the range is contiguous}\newline
\verb~>     }~\newline
\verb~>   }~\par

\verb!model! should be in the range 1 to 3\par
\verb~>   stopifnot(model %in% 1:3)~\newline
\verb~>   if (model==1){~\newline
\verb~>     message("Alas, Model 1 is not implemented yet. Returning zippedidooda (NULL)")~\newline
\verb~>     return(NULL)~\newline
\verb~>   }~\par
Convert time and site to factors, if they're not yet\par
\verb~>   if (any(class(time.id) %in% c("integer","numeric"))) time.id <- ordered(time.id)~\newline
\verb~>   ntime = length(levels(time.id))~\par

\verb~>   if (class(site.id) %in% c("integer","numeric")) site.id <- factor(site.id)~\newline
\verb~>   nsite = length(levels(site.id))~\par

Create observation matrix $f$.
Convert the data from a vector representation to a matrix representation.
It's OK to have missing site/time combinations; these will automatically
translate to NA values.\par
\verb~>   f <- matrix(0, nsite, ntime)  ~{\sffamily\# ??? check if we shoud not use NA instead of 0!!!}\newline
\verb~>   rows <- as.integer(site.id)  ~{\sffamily\# `site.id' is a factor, thus this results in $1\ldots I$.}\newline
\verb~>   cols <- as.integer(time.id)  ~{\sffamily\# idem, $1 \ldots J$.}\newline
\verb~>   idx <- (cols-1)*nsite+rows  ~{\sffamily\# Create column-major linear index from row/column subscripts.}\newline
\verb~>   f[idx] <- count  ~{\sffamily\# ... such that we can paste all data into the right positions}\par

Create similar matrices for all covariates\par
\verb~>   if (use.covars) {~\newline
\verb~>     cvmat <- list()~\newline
\verb~>     for (i in 1:ncovar) {~\newline
\verb~>       cv = covars[[i]]~\newline
\verb~>       m <- matrix(NA, nsite, ntime)~\newline
\verb~>       m[idx] <- cv~\newline
\verb~>       cvmat[[i]] <- m~\newline
\verb~>     }~\newline
\verb~>   }~\par

We often need some specific subset of the data, e.g.\ all observations for site 3.
These are conveniently found by combining the following indices:\par
\verb~>   observed <- is.finite(f)  ~{\sffamily\# Flags observed (TRUE) / missing (FALSE) data}\newline
\verb~>   site <- as.vector(row(f))  ~{\sffamily\# Internal site identifiers are the row numbers of the original matrix.}\newline
\verb~>   time <- as.vector(col(f))  ~{\sffamily\# Idem for time points.}\newline
\verb~>   nobs <- rowSums(observed)  ~{\sffamily\# Number of actual observations per site}\par

For model 2, we do not allow for changepoints $<1$ or $\geq J$. At the same time,
a changepoint $1$ must be present\par
\verb~>   if (model==2) {~\newline
\verb~>     if (length(changepoints)==0) {~\newline
\verb~>       use.changepoints <- FALSE  ~{\sffamily\# Pretend we're not using changepoints at all}\newline
\verb~>       changepoints <- 1L  ~{\sffamily\# but internally use them nevertheless}\newline
\verb~>     } else {~\newline
\verb~>       use.changepoints <- TRUE~\newline
\verb~>       stopifnot(all(changepoints>=1L))~\newline
\verb~>       stopifnot(all(changepoints<ntime))~\newline
\verb~>       stopifnot(all(diff(changepoints)>0))~\newline
\verb~>       if (changepoints[1]!=1L) changepoints = c(1L, changepoints)~\newline
\verb~>     }~\newline
\verb~>   }~\par

For model 3, changepoints are not allowed
TODO: proper error msg "No Changepoints allowed with model 3"\par
\verb~>   if (model==3) stopifnot(length(changepoints)==0)~\par

We make use of the generic model structure
$$ \log\mu = A\alpha + B\beta $$
where design matrices $A$ and $B$ both have $IJ$ rows.
For efficiency reasons the model estimation algorithm works on a per-site basis.
There is thus no need to store these full matrices. Instead, $B$ is constructed as a
smaller matrix that is valid for any site, and $A$ is not used at all.\par

Create matrix $B$, which is model-dependent.\par
\verb~>   if (model==2) {~\newline
\verb~>     ncp  <-  length(changepoints)~\newline
\verb~>     J <- ntime~\newline
\verb~>     B <- matrix(0, J, ncp)~\newline
\verb~>     for (i in 1:ncp) {~\newline
\verb~>       cp1  <-  changepoints[i]~\newline
\verb~>       cp2  <-  ifelse(i<ncp, changepoints[i+1], J)~\newline
\verb~>       if (cp1>1) B[1:(cp1-1), i]  <-  0~\newline
\verb~>       B[cp1:cp2,i]  <-  0:(cp2-cp1)~\newline
\verb~>       if (cp2<J) B[(cp2+1):J,i]  <-  B[cp2,i]~\newline
\verb~>     }~\newline
\verb~>   } else if (model==3) {~\par
Model 3 in it's canonical form uses a single time parameter $\gamma$ per time step,
so design matrix $B$ is essentially a $J\times$J identity matrix.
Note, hoewever, that by definition $\gamma_1=0$, so effectively there are $J-1$ $\gamma$-values to consider.
As a consequence, the first column is deleted.\par
\verb~>     B <- diag(ntime)  ~{\sffamily\# Construct $J\times$J identity matrix}\newline
\verb~>     B <- B[ ,-1]  ~{\sffamily\# Remove first column}\newline
\verb~>   }~\par

optionally add covariates. Each covar class (except class 1) adds an extra copy of $B$,
where rows are cleared if that site/time combi does not participate in\par
\verb~>   if (use.covars) {~\newline
\verb~>     cvmask <- list()~\newline
\verb~>     for (cv in 1:ncovar) {~\newline
\verb~>       cvmask[[cv]] = list()~\newline
\verb~>       for (cls in 2:nclass[cv]) {~\newline
\verb~>         cvmask[[cv]][[cls]] <- list()~\newline
\verb~>         for (i in 1:nsite) {~\newline
\verb~>           cvmask[[cv]][[cls]][[i]] <- which(cvmat[[cv]][i, ]!=cls)~\newline
\verb~>         }~\newline
\verb~>       }~\newline
\verb~>     }~\par
The amount of extra parameter sets is the total amount of covarariate classes
minus the number of covariates (because class 1 does not add extra params)\par
\verb~>     num.extra.beta.sets <- sum(nclass-1)~\newline
\verb~>   }~\par

When we use covariates, $B$ is site-specific. We thus define a function to make
the proper $B$ for each site $i$\par

\verb~>   B0 <- B  ~{\sffamily\# The "standard" B}\newline
\verb~>   rm(B)~\newline
\verb~>   make.B <- function(i, debug=FALSE) {~\newline
\verb~>     if (debug) { printf("make.B(%i): B0:", i); str(B0) }~\newline
\verb~>     if (model==2 || model==3) {~\newline
\verb~>       if (use.covars) {~\par
Model 2 with covariates. Add a copy of B for each covar class\par
\verb~>         Bfinal <- B0~\newline
\verb~>         for (cv in ncovar) {~\newline
\verb~>           if (debug) printf("adding covar %d\n", cv)~\newline
\verb~>           for (cls in 2:nclass[cv]) {~\newline
\verb~>             if (debug) printf("adding class %d\n", cls)~\newline
\verb~>             Btmp <- B0~\newline
\verb~>             mask <- cvmask[[cv]][[cls]][[i]]~\newline
\verb~>             if (length(mask)>0) Btmp[mask, ] = 0~\newline
\verb~>             Bfinal <- cbind(Bfinal, Btmp)~\newline
\verb~>           }~\newline
\verb~>         }~\newline
\verb~>       } else {~\par
Model 2 without covariates. Just the normal B\par
\verb~>         Bfinal = B0~\newline
\verb~>       }~\newline
\verb~>     } else if (model==3) {~\newline
\verb~>       Bfinal = B0~\newline
\verb~>     }~\newline
\verb~>     if (debug) { printf("make.B(%i): Bfinal:", i); str(Bfinal)}~\newline
\verb~>     Bfinal~\newline
\verb~>   }~\par



\subsection{Setup parameters and state variables}\par

Parameter $\alpha$ has a unique value for each site.\par
\verb~>   alpha <- matrix(0, nsite,1)  ~{\sffamily\# Store as column vector}\par

Parameter $\beta$ is model dependent.\par
\verb~>   if (model==2) {~\par
For model 2 we have one $\beta$ per change points\par
\verb~>     nbeta <- length(changepoints)~\newline
\verb~>   } else if (model==3) {~\par
For model 3, we have one $\beta$ per time $j>1$\par
\verb~>     nbeta = ntime-1~\newline
\verb~>   }~\par
If we have covariates, $\beta$'s are repeated for each covariate class $>1$.\par
\verb~>   nbeta0 <- nbeta  ~{\sffamily\# Number of `baseline' (i.e., without covariates) $\beta$'s.}\newline
\verb~>   if (use.covars) {~\newline
\verb~>     nbeta <- nbeta0 * (sum(nclass-1)+1)~\newline
\verb~>   }~\par

All $\beta_j$ are initialized at 0, to reflect no trend (model 2) or no time effects (model 3)\par
\verb~>   beta <- matrix(0, nbeta,1)  ~{\sffamily\# Store as column vector}\par

Variable $\mu$ holds the estimated counts.\par
\verb~>   mu <- matrix(0, nsite, ntime)~\par




\subsection{Model estimation}\par

TRIM estimates the model parameters $\alpha$ and $\beta$ in an iterative fashion,
so separate functions are defined for the updates of these and other variables needed.\par

\verb~>   ~{\sffamily\# 3 Site-parameters $\alpha$}\par
Update $\alpha_i$ using:
$$ \alpha_i^t = \log z_i' f_i - \log z_i' \exp(B_i \beta^{t-1}) $$
where vector $z$ contains just ones if autocorrelation and overdispersion are
ignored (i.e., Maximum Likelihood, ML),
or weights, when these are taken into account (i.e., Generalized Estimating
Equations, GEE).
In this case,
$$ z = \mu V^{-1} $$
with $V$ a covariance matrix (see Section~\ref{covariance}).\par
\verb~>   update_alpha <- function(method=c("ML","GEE")) {~\newline
\verb~>     for (i in 1:nsite) {~\newline
\verb~>       B = make.B(i)~\newline
\verb~>       f_i <- f[site==i & observed==TRUE]  ~{\sffamily\# vector}\newline
\verb~>       B_i <- B[observed[site==i], , drop=FALSE]~\newline
\verb~>       if (method=="ML") {  ~{\sffamily\# no covariance; $V_i = \diag{mu}$}\newline
\verb~>         z_t <- matrix(1, 1, nobs[i])~\newline
\verb~>       } else if (method=="GEE") {  ~{\sffamily\# Use covariance}\newline
\verb~>         mu_i = mu[site==i & observed==TRUE]~\newline
\verb~>         z_t <- mu_i %*% V_inv[[i]]  ~{\sffamily\# define correlation weights}\newline
\verb~>       } else stop("Can't happen")~\newline
\verb~>       alpha[i] <<- log(z_t %*% f_i) - log(z_t %*% exp(B_i %*% beta))~\newline
\verb~>     }~\newline
\verb~>   }~\par



\subsubsection{Time parameters $\beta$}\par

Estimates for parameters $\beta$ are improved by computing a change in $\beta$ and
adding that to the previous values:
$$ \beta^t = \beta^{t-1} - (i_b)^{-1} U_b^\ast \label{beta}$$
where $i_b$ is a derivative matrix (see Section~\ref{Hessian})
and $U_b^\ast$ is a Fisher Scoring matrix (see Section~\ref{Scoring}).
Note that the `improvement' as defined by \eqref{beta} can actually results in a decrease in model fit.
These cases are identified by measuring the model Likelihood Ratio (Eqn~\eqref{LR}).
If this measure increases, then smaller adjustment steps are applied.
This process is repeated until an actually improvement is found.\par
\verb~>   update_beta <- function(method=c("ML","GEE"))~\newline
\verb~>   {~\newline
\verb~>     update_U_i()  ~{\sffamily\# update Score $U_b$ and Fisher Information $i_b$}\par

Compute the proposed change in $\beta$.\par
\verb~>     dbeta  <-  -solve(i_b) %*% U_b~\par

This is the maximum update; if it results in an \emph{increased} likelihood ratio,
then we have to take smaller steps. First record the original state and likelihood.\par
\verb~>     beta0 <- beta~\newline
\verb~>     lik0  <- likelihood()~\newline
\verb~>     stepsize = 1.0~\newline
\verb~>     for (subiter in 1:7) {~\newline
\verb~>       beta <<- beta0 + stepsize*dbeta~\par

\verb~>       update_mu(fill=FALSE)~\newline
\verb~>       update_alpha(method)~\newline
\verb~>       update_mu(fill=FALSE)~\par

\verb~>       lik <- likelihood()~\newline
\verb~>       if (lik < lik0) break else stepsize <- stepsize / 2  ~{\sffamily\# Stop or try again}\newline
\verb~>     }~\newline
\verb~>     subiter~\newline
\verb~>   }~\par



\subsubsection{Covariance and autocorrelation \label{covariance}}\par

Covariance matrix $V_i$ is defined by
\begin{equation}
  V_i = \sigma^2 \sqrt{\diag{\mu}} R \sqrt{\diag{\mu}} \label{V1}
\end{equation}
where $\sigma^2$ is a dispersion parameter (Section~\ref{sig2})
and $R$ is an (auto)correlation matrix.
Both of these two elements are optional.
If the counts are perfectly Possion distributed, $\sigma^2=1$,
and if autocorrelation is disabled (i.e.\ counts are independent),
Eqn~\eqref{V1} reduces to
\begin{equation}
  V_i = \sigma^2 \diag{\mu} \label{V2}
\end{equation}\par
\verb~>   V_inv  <- vector("list", nsite)  ~{\sffamily\# Create storage space for $V_i^{-1}$.}\newline
\verb~>   Omega <- vector("list", nsite)~\par

\verb~>   update_V <- function(method=c("ML","GEE")) {~\newline
\verb~>     for (i in 1:nsite) {~\newline
\verb~>       mu_i <- mu[site==i & observed]~\newline
\verb~>       f_i  <- f[site==i & observed]~\newline
\verb~>       d_mu_i <- diag(mu_i, length(mu_i))  ~{\sffamily\# Length argument guarantees diag creation}\newline
\verb~>       if (method=="ML") {~\newline
\verb~>         V_i <- sig2 * d_mu_i~\newline
\verb~>       } else if (method=="GEE") {~\newline
\verb~>         idx <- which(observed[i, ])~\newline
\verb~>         R_i <- Rg[idx,idx]~\newline
\verb~>         V_i <- sig2 * sqrt(d_mu_i) %*% R_i %*% sqrt(d_mu_i)~\newline
\verb~>       } else stop("Can't happen")~\newline
\verb~>       V_inv[[i]] <<- solve(V_i) # Store $V^{-1}  ~{\sffamily\# for later use}\newline
\verb~>       Omega[[i]] <<- d_mu_i %*% V_inv[[i]] %*% d_mu_i  ~{\sffamily\# idem for $\Omega_i$}\newline
\verb~>     }~\newline
\verb~>   }~\par

The (optional) autocorrelation structure for any site $i$ is stored in $n_i\times n_i$ matrix $R_i$.
In case there are no missing values, $n_i=J$, and the `full' or `generic' autocorrelation matrix $R$ is expressed
as
\begin{equation}
R = \begin{pmatrix}
  1          & \rho       & \rho^2     & \cdots & \rho^{J-1} \\
  \rho       & 1          & \rho       & \cdots & \rho^{J-2} \\
  \vdots     & \vdots     & \vdots     & \ddots & \vdots     \\
  \rho^{J-1} & \rho^{J-2} & \rho^{J-3} & \cdots & 1
  \end{pmatrix}
\end{equation}
where $\rho$ is the lag-1 autocorrelation.\par
\verb~>   Rg <- diag(1, ntime)  ~{\sffamily\# default (no autocorrelation) value}\newline
\verb~>   update_R <- function() {~\newline
\verb~>     Rg <<- rho ^ abs(row(diag(ntime)) - col(diag(ntime)))~\newline
\verb~>   }~\par

Lag-1 autocorrelation parameter $\rho$ is estimated as
\begin{equation}
  \hat{\rho} = \frac{1}{n_{i,j,j+1}\hat{\sigma}^2} \left(\Sum_i^I\Sum_j^{J-1} r_{i,j}r_{i,j+1}) \right)
\end{equation}
where the summation is over observed pairs $i,j$--$i,j+1$, and $n_{i,j,j+1}$ is the number of pairs involved.
Again, both $\rho$ and $R$ are computes $\rho$ in a stepwise per-site fashion.
Also, site-specific autocorrelation matrices $R_i$ are formed by removing the rows and columns from $R$
corresponding with missing observations.\par
\verb~>   rho <- 0.0  ~{\sffamily\# default value (ML)}\newline
\verb~>   update_rho <- function() {~\par
First estimate $\rho$\par
\verb~>     rho   <-  0.0~\newline
\verb~>     count <-  0~\newline
\verb~>     for (i in 1:nsite) {~\newline
\verb~>       for (j in 1:(ntime-1)) {~\newline
\verb~>         if (observed[i,j] && observed[i,j+1]) {  ~{\sffamily\# short-circuit AND intended}\newline
\verb~>           rho <- rho + r[i,j] * r[i,j+1]~\newline
\verb~>           count <- count+1~\newline
\verb~>         }~\newline
\verb~>       }~\newline
\verb~>     }~\newline
\verb~>     rho <<- rho / (count * sig2)  ~{\sffamily\# compute and store in outer environment}\newline
\verb~>   }~\par



\subsubsection{Overdispersion. \label{sig2}}\par

Dispersion parameter $\sigma^2$ is estimated as
\begin{equation}
  \hat{\sigma}^2 = \frac{1}{n_f - n_\alpha - n_\beta} \sum_{i,j} r_{ij}^2
\end{equation}
where the $n$ terms are the number of observations, $\alpha$'s and $\beta$'s, respectively.
Summation is over the observed $i,j$ only.
and $r_{ij}$ are Pearson residuals (Section~\ref{r})\par
\verb~>   sig2 <- 1.0  ~{\sffamily\# default value (Maximum Likelihood case)}\newline
\verb~>   update_sig2 <- function() {~\newline
\verb~>     df <- sum(nobs) - length(alpha) - length(beta)  ~{\sffamily\# degrees of freedom}\newline
\verb~>     sig2 <<- sum(r^2, na.rm=TRUE) / df~\newline
\verb~>   }~\par



\subsubsection{Pearson residuals\label{r}}\par

Deviations between measured and estimated counts are quantified by the
Pearson residuals $r_{ij}$, given by
\begin{equation}
  r_{ij} = (f_{ij} - \mu_{ij}) / \sqrt{\mu_{ij}}
\end{equation}\par
\verb~>   r <- matrix(0, nsite, ntime)~\newline
\verb~>   update_r <- function() {~\newline
\verb~>     r[observed] <<- (f[observed]-mu[observed]) / sqrt(mu[observed])~\newline
\verb~>   }~\par



\subsubsection{Derivatives and GEE scores}\par

\label{Hessian}\label{Scoring}
Derivative matrix $i_b$ is defined as
\begin{equation}
  -i_b = \sum_i B_i' \left(\Omega_i - \frac{1}{d_i}\Omega_i z_i z_i' \Omega_i\right) B_i \label{i_b}
\end{equation}
where
\begin{equation}
  \Omega_i = \diag{\mu_i} V_i^{-1} \diag{\mu_i} \label{Omega_i}
\end{equation}
with $V_i$ the covariance matrix for site $i$, and
\begin{equation}
  d_i = z_i' \Omega_i z_i \label{d_i}
\end{equation}\par
\verb~>   i_b <- 0~\newline
\verb~>   U_b <- 0~\newline
\verb~>   update_U_i <- function() {~\newline
\verb~>     i_b <<- 0  ~{\sffamily\# Also store in outer environment for later retrieval}\newline
\verb~>     U_b <<- 0~\newline
\verb~>     for (i in 1:nsite) {~\newline
\verb~>       B = make.B(i)~\newline
\verb~>       mu_i <- mu[site==i & observed]~\newline
\verb~>       f_i  <- f[site==i & observed]~\newline
\verb~>       d_mu_i <- diag(mu_i, length(mu_i))  ~{\sffamily\# Length argument guarantees diag creation}\newline
\verb~>       ones <- matrix(1, nobs[i], 1)~\newline
\verb~>       d_i <- as.numeric(t(ones) %*% Omega[[i]] %*% ones)  ~{\sffamily\# Could use sum(Omega) as well...}\newline
\verb~>       B_i <- B[observed[site==i], ,drop=FALSE]~\newline
\verb~>       i_b <<- i_b - t(B_i) %*% (Omega[[i]] - (Omega[[i]] %*% ones %*% t(ones) %*% Omega[[i]]) / d_i) %*% B_i~\newline
\verb~>       U_b <<- U_b + t(B_i) %*% d_mu_i %*% V_inv[[i]] %*% (f_i - mu_i)~\newline
\verb~>     }~\newline
\verb~>   }~\par



\subsubsection{Count estimates}\par

Let's not forget to provide a function to update the modelled counts $\mu_{ij}$:
$$ \mu^t = \exp(A\alpha^t + B\beta^{t-1} - \log w) $$
where it is noted that we do not use matrix $A$. Instead, the site-specific
parameters $\alpha_i$ are used directly:
$$ \mu_i^t = \exp(\alpha_i^t + B\beta^{t-1} - \log w) $$\par
\verb~>   update_mu <- function(fill) {~\newline
\verb~>     for (i in 1:nsite) {~\newline
\verb~>       B = make.B(i)~\newline
\verb~>       mu[i, ] <<- exp(alpha[i] + B %*% beta)~\newline
\verb~>     }~\par
clear estimates for non-observed cases, if required.\par
\verb~>     if (!fill) mu[!observed] <<- 0.0~\newline
\verb~>   }~\par



\subsubsection{Likelihood}\par

\verb~>   likelihood <- function() {~\newline
\verb~>     lik <- 2*sum(f*log(f/mu), na.rm=TRUE)~\newline
\verb~>     lik~\newline
\verb~>   }~\par



\subsubsection{Convergence}\par

The parameter estimation algorithm iterated until convergence is reached.
`convergence' here is defined in a multivariate way: we demand convergence in
model paramaters $\alpha$ and $\beta$, model estimates $\mu$ and likelihood measure $L$.\par
\verb~>   new_par <- new_cnt <- new_lik <- NULL~\newline
\verb~>   old_par <- old_cnt <- old_lik <- NULL~\newline
\verb~>   check_convergence <- function(iter, crit=1e-5) {~\par

Collect new data for convergence test
(Store in outer environment to make them persistent)\par
\verb~>     new_par <<- c(as.vector(alpha), as.vector(beta))~\newline
\verb~>     new_cnt <<- as.vector(mu)~\newline
\verb~>     new_lik <<- likelihood()~\par

\verb~>     if (iter>1) {~\newline
\verb~>       max_par_change <- max(abs(new_par - old_par))~\newline
\verb~>       max_cnt_change <- max(abs(new_cnt - old_cnt))~\newline
\verb~>       max_lik_change <- max(abs(new_lik - old_lik))~\newline
\verb~>       conv_par <- max_par_change < crit~\newline
\verb~>       conv_cnt <- max_cnt_change < crit~\newline
\verb~>       conv_lik <- max_lik_change < crit~\newline
\verb~>       convergence <- conv_par && conv_cnt && conv_lik~\newline
\verb~>       rprintf(" Max change: %10e %10e %10e ", max_par_change, max_cnt_change, max_lik_change)~\newline
\verb~>     } else {~\newline
\verb~>       convergence = FALSE~\newline
\verb~>     }~\par

Today's new stats are tomorrow's old stats\par
\verb~>     old_par <<- new_par~\newline
\verb~>     old_cnt <<- new_cnt~\newline
\verb~>     old_lik <<- new_lik~\par

\verb~>     convergence~\newline
\verb~>   }~\par



\subsubsection{Main estimation procedure}\par

Now we have all the building blocks ready to start the iteration procedure.
We start `smooth', with a couple of Maximum Likelihood iterations
(i.e., not considering $\sigma^2\neq1$ or $\rho>0$), after which we move to on
GEE iterations if requested.\par

\verb~>   method    <- "ML"  ~{\sffamily\# start with Maximum Likelihood}\newline
\verb~>   final_method <- ifelse(serialcor || overdisp, "GEE", "ML")  ~{\sffamily\# optionally move on to GEE}\par

\verb~>   max_iter  <- 100  ~{\sffamily\# Maximum number of iterations allowed}\newline
\verb~>   conv_crit <- 1e-7~\newline
\verb~>   for (iter in 1:max_iter) {~\newline
\verb~>     rprintf("Iteration %d (%s)", iter, method)~\par

\verb~>     update_alpha(method)~\newline
\verb~>     update_mu(fill=FALSE)~\newline
\verb~>     if (method=="GEE") {~\newline
\verb~>       update_r()~\newline
\verb~>       if (overdisp)  update_sig2()~\newline
\verb~>       if (serialcor) update_rho()~\newline
\verb~>       update_R()~\newline
\verb~>     }~\newline
\verb~>     update_V(method)~\newline
\verb~>     subiters <- update_beta(method)~\newline
\verb~>     rprintf(", %d subiters", subiters)~\newline
\verb~>     rprintf(", lik=%.3f", likelihood())~\newline
\verb~>     if (overdisp)  rprintf(", sig^2=%.5f", sig2)~\newline
\verb~>     if (serialcor) rprintf(", rho=%.5f;", rho)~\par

\verb~>     convergence <- check_convergence(iter)~\par

\verb~>     if (convergence && method==final_method) {~\newline
\verb~>       rprintf("\nConvergence reached\n")~\newline
\verb~>       break~\newline
\verb~>     } else if (convergence) {~\newline
\verb~>       rprintf("\nChanging ML --> GEE\n")~\newline
\verb~>       method = "GEE"~\newline
\verb~>     } else {~\newline
\verb~>       rprintf("\n")~\newline
\verb~>     }~\newline
\verb~>   }~\par


If we reach the preset maximum number of iterations, we clearly have not reached
convergence.\par
\verb~>   if (iter==max_iter) stop("No convergence reached.")~\par

Run the final model\par
\verb~>   update_mu(fill=TRUE)~\par

Covariance matrix\par
\verb~>   var_beta <- -solve(i_b)~\par



\subsection{Imputation}\par

The imputation process itself is trivial: just replace all missing observations
$f_{i,j}$ by the model-based estimates $\mu_{i,j}$.\par
\verb~>   imputed <- ifelse(observed, f, mu)~\par




\subsection{Output and postprocessing}\par

Measured, modelled and imputed count data are stored in a TRIM output object,
together with parameter values and other usefull information.\par

\verb~>   z <- list(title=title, f=f, nsite=nsite, ntime=ntime, nbeta0=nbeta0,~\newline
\verb~>             covars=covars, ncovar=ncovar,~\newline
\verb~>             model=model, changepoints=changepoints,~\newline
\verb~>             mu=mu, imputed=imputed, alpha=alpha, beta=beta, var_beta=var_beta)~\newline
\verb~>   if (use.covars) {~\newline
\verb~>     z$ncovar <- ncovar  ~{\sffamily\# todo: eliminate?}\newline
\verb~>     z$nclass <- nclass~\newline
\verb~>   }~\newline
\verb~>   class(z) <- "trim"~\par

Several kinds of statistics can now be computed, and added to this output object.\par



\subsubsection{Overdispersion and Autocorrelation}\par

\verb~>   z$sig2 <- ifelse(overdisp, sig2, NA)~\newline
\verb~>   z$rho  <- ifelse(serialcor, rho,  NA)~\par



\subsubsection{Coefficients and uncertainty}\par

\verb~>   if (model==2) {~\newline
\verb~>     se_beta  <- sqrt(diag(var_beta))~\par

\verb~>     ncp = length(changepoints)~\newline
\verb~>     coefs = data.frame(~\newline
\verb~>       from   = changepoints,~\newline
\verb~>       upto   = if (ncp==1) ntime else c(changepoints[2:ncp], ntime),~\newline
\verb~>       add    = beta,~\newline
\verb~>       se_add = se_beta,~\newline
\verb~>       mul    = exp(beta),~\newline
\verb~>       se_mul = exp(beta) * se_beta~\newline
\verb~>     )~\par

\verb~>     if (use.covars) {~\par
Add some prefix columns with covariate and factor ID
Note that we have to specify all covariate levels here, to prevent them
from being to converted to NA later\par
\verb~>       prefix <- data.frame(covar = factor("baseline",levels=c("baseline", names(covars))),~\newline
\verb~>                            cat   = 0)~\newline
\verb~>       coefs <- cbind(prefix, coefs)~\newline
\verb~>       idx = 1:nbeta0~\newline
\verb~>       for (i in 1:ncovar) {~\newline
\verb~>         for (j in 2:nclass[i]) {~\newline
\verb~>           idx <- idx + nbeta0~\newline
\verb~>           coefs$covar[idx]  <- names(covars)[i]~\newline
\verb~>           coefs$cat[idx]    <- j~\newline
\verb~>         }~\newline
\verb~>       }~\newline
\verb~>     }~\par

\verb~>     z$coefficients <- coefs~\newline
\verb~>   }  ~{\sffamily\# if model==2}\par

\verb~>   if (model==3) {~\par
Model coefficients are output in two types; as additive parameters:
$$ \log\mu_{ij} = \alpha_i + \gamma_j $$
and as multiplicative parameters:
$$ \mu_{ij} = a_i g_j $$
where $a_i=e^{\alpha_i}$ and $g_j = e^{\gamma_j}$.\par

For the first time point, $\gamma_1=0$ by definition.
So we have to add values of 0 for the baseline case and each covariate category $>1$, if any.\par
\verb~>     #gamma <- matrix(beta, nrow=nbeta0)  ~{\sffamily\# Each covariate category in a column}\newline
\verb~>     #gamma <- rbind(0, gamma)  ~{\sffamily\# Add row of 0's for first time point}\newline
\verb~>     #gamma <- matrix(gamma, ncol=1)  ~{\sffamily\# Cast back into a column vector}\newline
\verb~>     gamma <- beta~\newline
\verb~>     g     <- exp(gamma)~\par

Parameter uncertainty is expressed as standard errors.
For the additive parameters $\gamma$, the variance is estimated as
$$ \var{\gamma} = (-i_b)^{-1} $$\par
\verb~>     var_gamma <-  -solve(i_b)~\par
Finally, we compute the standard error as $\se{\gamma} = \sqrt{\diag{\var{\gamma}}}$\par
\verb~>     se_gamma  <-  sqrt(diag(var_gamma))~\par

The standard error of the multiplicative parameters $g_j$ is opproximated by
using the delta method, which is based on a Taylor expansion:
\begin{equation}
  \var{f(\theta)} = \left(f'(\theta)\right)^2 \var{\theta}
\end{equation}
which for $f(\theta)=e^\theta$ translates to
$$ \var{g} = \var{e^{\gamma}} = e^{2\gamma} \var{\gamma} $$
leading to
$$ \se{g} = e^{\gamma} \se{\gamma} = g \se{\gamma} $$\par
\verb~>     se_g <- g * se_gamma~\par

Baseline coefficients.
Note that, because $\gamma_1\equiv0$, it was not estimated,
and as a results $j=1$ was not incuded in $i_b$, nor in $\var{gamma}$ as computed above.
We correct this by adding the `missing' 0 (or 1 for multiplicative parameters) during output\par
\verb~>     idx = 1:nbeta0~\newline
\verb~>     coefs <- data.frame(~\newline
\verb~>       time   = 1:ntime,~\newline
\verb~>       add    = c(0, gamma[idx]),~\newline
\verb~>       se_add = c(0, se_gamma[idx]),~\newline
\verb~>       mul    = c(1, g[idx]),~\newline
\verb~>       se_mul = c(0, g[idx] * se_gamma[idx])~\newline
\verb~>     )~\par

Covariate categories ($>1$)\par
\verb~>     if (use.covars) {~\newline
\verb~>       prefix = data.frame(covar="baseline", cat=0)~\newline
\verb~>       coefs <- cbind(prefix, coefs)~\newline
\verb~>       for (i in 1:ncovar) {~\newline
\verb~>         for (j in 2:nclass[i]) {~\newline
\verb~>           idx <- idx + nbeta0~\newline
\verb~>           df <- data.frame(~\newline
\verb~>             covar  = names(covars)[i],~\newline
\verb~>             cat    = j,~\newline
\verb~>             time   = 1:ntime,~\newline
\verb~>             add    = c(0, gamma[idx]),~\newline
\verb~>             se_add = c(0, se_gamma[idx]),~\newline
\verb~>             mul    = c(1, g[idx]),~\newline
\verb~>             se_mul = c(0, g[idx] * se_gamma[idx])~\newline
\verb~>           )~\newline
\verb~>           coefs <- rbind(coefs, df)~\newline
\verb~>         }~\newline
\verb~>       }~\newline
\verb~>     }~\par

\verb~>     z$coefficients <- coefs~\newline
\verb~>   }  ~{\sffamily\# if model==3}\par




\subsubsection{Time totals}\par

Recompute Score matrix $i_b$ with final $\mu$'s\par
\verb~>   ib <- 0~\newline
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     B <- make.B(i)~\newline
\verb~>     mu_i <- mu[site==i & observed]~\newline
\verb~>     n_i <- length(mu_i)~\newline
\verb~>     d_mu_i <- diag(mu_i, n_i)  ~{\sffamily\# Length argument guarantees diag creation}\newline
\verb~>     OM <- Omega[[i]]~\newline
\verb~>     d_i <- sum(OM)  ~{\sffamily\# equivalent with z' Omega z, as in the TRIM manual}\newline
\verb~>     B_i <- B[observed[site==i], ,drop=FALSE]~\newline
\verb~>     om <- colSums(OM)~\newline
\verb~>     OMzzOM <- om %*% t(om)  ~{\sffamily\# equivalent with OM z z' OM, as in the TRIM manual}\newline
\verb~>     term <- t(B_i) %*% (OM - (OMzzOM) / d_i) %*% B_i~\newline
\verb~>     ib <- ib - term~\newline
\verb~>   }~\par



Matrices E and F take missings into account\par
\verb~>   E <- -ib~\par

\verb~>   nbeta <- length(beta)~\newline
\verb~>   F <- matrix(0, nsite, nbeta)~\newline
\verb~>   d <- numeric(nsite)~\newline
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     B <- make.B(i)~\newline
\verb~>     d[i] <- sum(Omega[[i]])~\newline
\verb~>     w_i <- colSums(Omega[[i]])~\newline
\verb~>     B_i <- B[observed[site==i], ,drop=FALSE]~\newline
\verb~>     F_i <- (t(w_i) %*% B_i) / d[i]~\newline
\verb~>     F[i, ] <- F_i~\newline
\verb~>   }~\par

Matrices G and H are for all mu's\par

\verb~>   GddG <- matrix(0, ntime,ntime)~\newline
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     for (j in 1:ntime) for (k in 1:ntime) {~\newline
\verb~>       GddG[j,k] <- GddG[j,k] + mu[i,j]*mu[i,k]/d[i]~\newline
\verb~>     }~\newline
\verb~>   }~\par

\verb~>   GF <- matrix(0, ntime, nbeta)~\newline
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     for (j in 1:ntime) for (k in 1:nbeta)  {~\newline
\verb~>       GF[j,k] <- GF[j,k] + mu[i,j] * F[i,k]~\newline
\verb~>     }~\newline
\verb~>   }~\par

\verb~>   H <- matrix(0, ntime, nbeta)~\newline
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     B <- make.B(i)~\newline
\verb~>     for (k in 1:nbeta) for (j in 1:ntime) {~\newline
\verb~>       H[j,k]  <- H[j,k] + B[j,k] * mu[i,j]~\newline
\verb~>     }~\newline
\verb~>   }~\par

\verb~>   GFminH <- GF - H~\par

All building blocks are ready. Use them to compute the variance\par
\verb~>   var_tt_mod <- GddG + GFminH %*% solve(E) %*% t(GFminH)~\par

To compute the variance of the time totals of the imputed data, we first
substract the contribution due tp te observations, as computed by above scheme,
and recplace it by the contribution due to the observations, as resulting from the
covariance matrix.\par
\verb~>   muo = mu  ~{\sffamily\# 'observed' $\mu$'s}\newline
\verb~>   muo[!observed] = 0 #  ~{\sffamily\# erase estimated $\mu$'s}\par

\verb~>   GddG <- matrix(0, ntime,ntime)~\newline
\verb~>   for (i in 1:nsite) if (nobs[i]>0) {~\newline
\verb~>     for (j in 1:ntime) for (k in 1:ntime) {~\newline
\verb~>       GddG[j,k] <- GddG[j,k] + muo[i,j]*muo[i,k]/d[i]~\newline
\verb~>     }~\newline
\verb~>   }~\par

\verb~>   GF <- matrix(0, ntime, nbeta)~\newline
\verb~>   for (i in 1:nsite) if (nobs[i]>0) {~\newline
\verb~>     for (j in 1:ntime) for (k in 1:nbeta)  {~\newline
\verb~>       GF[j,k] <- GF[j,k] + muo[i,j] * F[i,k]~\newline
\verb~>     }~\newline
\verb~>   }~\par

\verb~>   H <- matrix(0, ntime, nbeta)~\newline
\verb~>   for (i in 1:nsite) if (nobs[i]>0) {~\newline
\verb~>     B <- make.B(i)~\newline
\verb~>     for (k in 1:nbeta) for (j in 1:ntime) {~\newline
\verb~>       H[j,k]  <- H[j,k] + B[j,k] * muo[i,j]~\newline
\verb~>     }~\newline
\verb~>   }~\par

\verb~>   GFminH <- GF - H~\par

\verb~>   var_tt_obs_old <- GddG + GFminH %*% solve(E) %*% t(GFminH)~\par

Now compute the variance due to observations\par
\verb~>   var_tt_obs_new = matrix(0, ntime, ntime)~\newline
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     if (serialcor) {~\newline
\verb~>       srdu = sqrt(diag(muo[i, ]))~\newline
\verb~>       V = sig2 * srdu %*% Rg %*% srdu~\newline
\verb~>     } else {~\newline
\verb~>       V = sig2 * diag(muo[i, ])~\newline
\verb~>     }~\newline
\verb~>     var_tt_obs_new = var_tt_obs_new + V~\newline
\verb~>   }~\par

Combine\par
\verb~>   var_tt_imp = var_tt_mod - var_tt_obs_old + var_tt_obs_new~\par

Time totals of the model, and it's standard error\par
\verb~>   tt_mod    <- colSums(mu)~\newline
\verb~>   se_tt_mod <- round(sqrt(diag(var_tt_mod)))~\par

\verb~>   tt_imp     <- colSums(imputed)~\newline
\verb~>   se_tt_imp <- round(sqrt(diag(var_tt_imp)))~\par

Store in TRIM output\par
\verb~>   z$tt_mod <- tt_mod~\newline
\verb~>   z$tt_imp <- tt_imp~\newline
\verb~>   z$var_tt_mod <- var_tt_mod~\newline
\verb~>   z$var_tt_imp <- var_tt_imp~\par

\verb~>   z$time.totals <- data.frame(~\newline
\verb~>     time    = 1:ntime,~\newline
\verb~>     model   = round(tt_mod),~\newline
\verb~>     se_mod  = se_tt_mod,~\newline
\verb~>     imputed = round(tt_imp),~\newline
\verb~>     se_imp  = se_tt_imp~\newline
\verb~>   )~\par





\subsubsection{Reparameterisation of Model 3}\par

Here we consider the reparameterization of the time-effects model in terms of
a model with a linear trend and deviations from this linear trend for each time point.
The time-effects model is given by
\begin{equation}
  \log\mu_{ij}=\alpha_i+\gamma_j,
\end{equation}
with $\gamma_j$ the effect for time $j$ on the log-expected counts and $\gamma_1=0$. This reparameterization can be expressed as
\begin{equation}
  \log\mu_{ij}=\alpha^*_i+\beta^*d_j+\gamma^*_j,
\end{equation}
with $d_j=j-\bar{j}$ and $\bar{j}$ the mean of the integers $j$ representing
the time points.
The parameter $\alpha^*_i$ is the intercept and the parameter $\beta^*$ is
the slope of the least squares regression line through the $J$ log-expected
time counts in site $i$ and  $\gamma^*_j$ can be seen as the residuals of this
linear fit.
From regression theory we have that the `residuals'"'  $\gamma^*_j$ sum to zero
and are orthogonal to the explanatory variable, i.e.
\begin{equation}
  \sum_j\gamma^*_j = 0 \quad \text{and} \quad \sum_jd_j\gamma^*_j = 0. \label{constraints}
\end{equation}
Using these constraints we obtain the equations:
\begin{gather}
  \log\mu_{ij}           = \alpha^*_i+\beta^*d_j+\gamma^*_j=\alpha_i+\gamma_j  \label{repar1}\\
  \sum_j \log\mu_{ij}    = J\alpha^*_j = J\alpha_i+\sum_j\gamma_j \label{repar2}\\
  \sum_j d_j\log\mu_{ij} = \beta^*\sum_jd^2_j = \sum_jd_j\gamma_j \label{repar3},
\end{gather}
where \eqref{repar1} is the re-parameterization equation itself and \eqref{repar2}
and \eqref{repar3} are obtained by using the constraints~\eqref{constraints}\par

From \eqref{repar2} we have that $\alpha^*_i=\alpha_i+\frac{1}{J}\sum_j\gamma_j$.
Now, by using the equations \eqref{repar1} thru \eqref{repar3} and defining
$D=\sum_jd^2_j$, we can express the parameters $\beta^*$ and $\gamma^*$ as
functions of the parameters $\gamma$ as follows:
\begin{align}
  \label{betaster}
  \beta^* &=\frac{1}{D}\sum_jd_j\gamma_j,\\ \nonumber
  \label{gammaster}
  \gamma^*_j &= \alpha_i+\gamma_j-\alpha^*_i-\beta^*d_j  \quad (\text{using (5)})\\ \nonumber
  &=\alpha_i-\left( \alpha_i+\frac{1}{J}\sum_j\gamma_j\right) +\gamma_j-d_j\frac{1}{D}\sum_jd_j\gamma_j \\
  &=\gamma_j-\frac{1}{J}\sum_j\gamma_j-d_j\frac{1}{D}\sum_jd_j\gamma_j.
\end{align}
Since $\beta^*$ and $\gamma^*_j$ are linear functions of the parameters $\gamma_j$
they can be expressed in matrix notation by
\begin{equation}
  \left ( \begin{array} {c}
         \beta^* \\
         \boldsymbol{\gamma}^*
  \end{array} \right ) = \mathbf{T}\boldsymbol{\gamma},
\end{equation}
with $\boldsymbol{\gamma}^*=(\gamma^*_1,\ldots,\gamma^*_J)^T$,
$\boldsymbol{\gamma}=(\gamma_1,\ldots,\gamma_J)^T$ and $\mathbf{T}$
the $(J+1) \times J$ transformation matrix that transforms
$\boldsymbol{\gamma}$ to  $\left (\beta^*,(\boldsymbol{\gamma}^*)^T\right)^T$.
From \eqref{betaster} and \eqref{gammaster} it follows that the elements of
$\mathbf{T}$ are given by:
\begin{align}
  \label{matrixT} \nonumber
  &\mathbf{T}_{(1,j)}=\frac{d_j}{D} &\quad (i=1,j=1,\ldots,J)\\ \nonumber
  &\mathbf{T}_{(i,j)}=1-\frac{1}{J}-\frac{1}{D}d_{i-1}d_j &\quad(i=2,\ldots,J+1,j=1,\ldots,J,i-1=j)\\ \nonumber
  &\mathbf{T}_{(i,j)}=-\frac{1}{J}-\frac{1}{D}d_{i-1}d_j &\quad(i=2,\ldots,J+1,j=1,\ldots,J,i-1 \neq j)
\end{align}\par

\verb~>   if (model==3 && !use.covars) {~\par

\verb~>     TT <- matrix(0, ntime+1, ntime)~\newline
\verb~>     J <- ntime~\newline
\verb~>     j <- 1:J; d <- j - mean(j)  ~{\sffamily\# i.e, $ d_j = j-\frac{1}{J}\sum_j j$}\newline
\verb~>     D <- sum(d^2)  ~{\sffamily\# i.e., $ D = \sum_j d_j^2$}\newline
\verb~>     TT[1, ] <- d / D~\newline
\verb~>     for (i in 2:(J+1)) for (j in 1:J) {~\newline
\verb~>       if (i-1 == j) {~\newline
\verb~>         TT[i,j] <- 1 - (1/J) - d[i-1]*d[j]/D~\newline
\verb~>       } else {~\newline
\verb~>         TT[i,j] <-   - (1/J) - d[i-1]*d[j]/D~\newline
\verb~>       }~\newline
\verb~>     }~\par

\verb~>     gstar <- TT %*% c(0, gamma)  ~{\sffamily\# Add the implicit $\gamma_1=0$}\newline
\verb~>     bstar <- gstar[1]~\newline
\verb~>     gstar <- gstar[2:(J+1)]~\par

The covariance matrix of the transformed parameter vector can now be obtained
from the covariance matrix $\mathbf{T}\boldsymbol{\gamma}$ of $\boldsymbol{\gamma}$ as
\begin{equation}
  V\left( \begin{array} {c} \beta^* \\\boldsymbol{\gamma}^* \end{array} \right)
  = \mathbf{T}V(\boldsymbol{\gamma})\mathbf{T}^T
\end{equation}\par

\verb~>     var_gstar <- TT %*% rbind(0,cbind(0,var_gamma)) %*% t(TT)  ~{\sffamily\# Again, $\gamma_1=0$}\newline
\verb~>     se_bstar  <- sqrt(diag(var_gstar))[1]~\newline
\verb~>     se_gstar  <- sqrt(diag(var_gstar))[2:(ntime+1)]~\par

\verb~>     z$gstar <- gstar~\newline
\verb~>     z$var_gstar <- var_gstar~\par

\verb~>     z$linear.trend <- data.frame(~\newline
\verb~>       Additive       = bstar,~\newline
\verb~>       std.err        = se_bstar,~\newline
\verb~>       Multiplicative = exp(bstar),~\newline
\verb~>       std.err.       = exp(bstar) * se_bstar,~\newline
\verb~>       row.names      = "Slope",~\newline
\verb~>       check.names    = FALSE)~\par

Deviations from the linear trend\par
\verb~>     z$deviations <- data.frame(~\newline
\verb~>       Time       = 1:ntime,~\newline
\verb~>       Additive   = gstar,~\newline
\verb~>       std.err.   = se_gstar,~\newline
\verb~>       Multiplicative = exp(gstar),~\newline
\verb~>       std.err.   = exp(gstar) * se_gstar,~\newline
\verb~>       check.names = FALSE~\newline
\verb~>     )~\par

\verb~>   }~\par




\subsection{Return results}\par

The TRIM result is returned to the user\ldots\par
\verb~>   printf("(Exiting workhorse function)\n")~\newline
\verb~>   z~\newline
\verb~> }~\par
\ldots which ends the main TRIM function.


\section{Goodness of fit}\par

The goodness-of-fit of the model is assessed using three statistics:
Chi-squared, Likelihood Ratio and Aikaike Information Content.\par



\subsection{Computation}\par

Here we define `gof' as a S3 generic function\par
\verb~> gof <- function(x) UseMethod("gof")~\par

Here is a simple wrapper function for TRIM output lists.\par
\verb~> gof.trim <- function(x) {~\newline
\verb~>   printf("gof.trim() called\n")~\newline
\verb~>   stopifnot(class(x)=="trim")~\newline
\verb~>   gof.numeric(x$f, x$mu, x$alpha, x$beta)~\newline
\verb~> }~\par

Here is the workhorse function\par

\verb~> gof.numeric <- function(f, mu, alpha, beta) {~\newline
\verb~>   printf("gof.numeric() called\n")~\newline
\verb~>   observed <- is.finite(f)~\par

The $\chi^2$ (Chi-square) statistic is given by
\begin{equation}
  \chi^2 = \sum_{ij}\frac{f_{i,j}-\mu_{i,j}}{\mu_{i,j}}
\end{equation}
where the summation is over the observed $i,j$'s only.
Significance is assessed by comparing against a $\chi^2$ distribution with
$df$ degrees of freedom, equal to the number of observations
minus the total number of parameters involved, i.e.\
$df = n_f - n_\alpha - n_\beta$.\par
\verb~>   chi2 <- sum((f-mu)^2/mu, na.rm=TRUE)~\newline
\verb~>   df   <- sum(observed) - length(alpha) - length(beta)~\newline
\verb~>   p    <-  1 - pchisq(chi2, df=df)~\newline
\verb~>   chi2 <- list(chi2=chi2, df=df, p=p)  ~{\sffamily\# store in a list}\par

Similarly, the \emph{Likelihood ratio} (LR) is computed as
\begin{equation}
  \operatorname{LR} = 2\sum_{ij}f_{ij} \log\frac{f_{i,j}}{\mu_{i,j}} \label{LR}
\end{equation}
and again compared against a $\chi^2$ distribution.\par
\verb~>   LR <- 2 * sum(f * log(f / mu), na.rm=TRUE)~\newline
\verb~>   df <- sum(observed) - length(alpha) - length(beta)~\newline
\verb~>   p  <- 1 - pchisq(LR, df=df)~\newline
\verb~>   LR <- list(LR=LR, df=df, p=p)~\par

The Akaike Information Content (AIC) is related to the LR as:\par
\verb~>   AIC <- LR$LR - 2*LR$df~\par

Output all goodness-of-fit measures in a single list\par
\verb~>   structure(list(chi2=chi2, LR=LR, AIC=AIC), class="trim.gof")~\newline
\verb~> }~\par



\subsection{Printing}\par

A simple printing function is provided that mimics TRIM for Windows output.\par

\verb~> print.trim.gof <- function(x) {~\newline
\verb~>   stopifnot(class(x)=="trim.gof")~\par
print welcome message\par
\verb~>   cat(sprintf("Goodness of fit\n"))~\par

print $\chi^2$ results\par
\verb~>   with(x$chi2,~\newline
\verb~>        printf("%24s = %.2f, df=%d, p=%.4f\n", "Chi-square", chi2, df, p))~\par

idem, Likelihood ratio\par
\verb~>   with(x$LR,~\newline
\verb~>        printf("%24s = %.2f, df=%d, p=%.4f\n", "Likelihood Ratio", LR, df, p))~\par

idem, Akaike Information Content\par
\verb~>   with(x,~\newline
\verb~>        printf("%24s = %.2f\n", "AIC (up to a constant)", AIC))~\newline
\verb~> }~\par


\section{TRIM postprocessing functions}\par



\subsection{Data summary}\par



\subsubsection{extract}\par

\verb~> summary.TRIMdata <- function(x)~\newline
\verb~> {~\newline
\verb~>   stopifnot(class(x)=="TRIMdata")~\par

Collect covariate data\par
\verb~>   covar_cols <- ifelse(x$weight, 5,4) : ncol(x$df)~\newline
\verb~>   covars <- data.frame(col = covar_cols,~\newline
\verb~>                        name = names(x$df)[covar_cols],~\newline
\verb~>                        levels = sapply(x$df[covar_cols], nlevels)~\newline
\verb~>   )~\par

Create summary output\par
\verb~>   out <- list(ncols=ncol(x$df), file=x$file, nsite=x$nsite, ntime=x$ntime,~\newline
\verb~>               missing=x$missing, covars=covars,~\newline
\verb~>               nzero=x$nzero, npos=x$npos, nobs=x$nobs, nmis=x$nmis,~\newline
\verb~>               ncount=x$ncount, totcount=x$totcount)~\newline
\verb~>   class(out) <- "summary.TRIMdata"~\newline
\verb~>   out~\newline
\verb~> }~\par

\verb~> dominant_sites <- function(x, threshold=10) {~\newline
\verb~>   stopifnot(class(x)=="TRIMdata")~\par

Compute site totals\par
\verb~>   ST <- ddply(x$df, .(site), summarize, total=sum(count, na.rm=TRUE))~\newline
\verb~>   ST$percent <- 100 * ST$total / x$totcount~\par

Dominate sites: more than 10% of total observations\par
\verb~>   DOM <- subset(ST, percent>threshold)~\par

Create summary output\par
\verb~>   out <- list(sites=DOM, threshold=threshold)~\newline
\verb~>   class(out) <- "trim.dom"~\newline
\verb~>   out~\newline
\verb~> }~\par

\verb~> average <- function(x)~\newline
\verb~> {~\newline
\verb~>   stopifnot(class(x)=="TRIMdata")~\par

Number of observations and mean count for each  time point can be computed
directly using the plyr tools.\par
\verb~>   out <- ddply(x$df, .(time), summarise,~\newline
\verb~>                observations=sum(is.finite(count)),~\newline
\verb~>                average=mean(count, na.rm=TRUE))~\newline
\verb~>   out$index <- out$average / out$average[1]~\newline
\verb~>   out~\newline
\verb~> }~\par



\subsubsection{print}\par

\verb~> print.summary.TRIMdata <- function(x)~\newline
\verb~> {~\newline
\verb~>   stopifnot(class(x)=="summary.TRIMdata")~\par

\verb~>   printf("\nThe following %d variables have been read from file: %s\n", x$ncols, x$file)~\newline
\verb~>   printf("1. %-20s number of values: %d\n", "Site", x$nsite)~\newline
\verb~>   printf("2. %-20s number of values: %d\n", "Time", x$ntime)~\newline
\verb~>   printf("3. %-20s missing = %d\n", "Count", x$missing)~\par
TODO: weight column\par
\verb~>   for (i in 1:nrow(x$covars)) {~\newline
\verb~>     printf("%d. %-20s number of values: %d\n", x$covars$col[i], x$covars$name[i], x$covars$levels[i])~\newline
\verb~>   }~\par

\verb~>   printf("\n")~\newline
\verb~>   printf("Number of observed zero counts      %8d\n", x$nzero)~\newline
\verb~>   printf("Number of observed positive counts  %8d\n", x$npos)~\newline
\verb~>   printf("Total number of observed counts     %8d\n", x$nobs)~\newline
\verb~>   printf("Number of missing counts            %8d\n", x$nmis)~\newline
\verb~>   printf("Total number of counts              %8d\n", x$ncount)~\newline
\verb~>   printf("\n")~\newline
\verb~>   printf("Total count                         %8d\n", x$totcount)~\par

\verb~> }~\par

\verb~> print.trim.dom <- function(dom) {~\newline
\verb~>   stopifnot(class(dom)=="trim.dom")~\par

\verb~>   printf("\nSites containing more than %d%% of the total count:\n", dom$threshold)~\newline
\verb~>   print(dom$sites, row.names=FALSE)~\newline
\verb~> }~\par




\subsection{Summary}\par

\verb~> summary.trim <- function(x) {~\newline
\verb~>   stopifnot(class(x)=="trim")~\par

\verb~>   if (is.finite(x$sig2) || is.finite(x$rho)) {~\newline
\verb~>     out = list(est.method="Generalised Estimating Equations")~\newline
\verb~>   } else {~\newline
\verb~>     out = list(est.method="Maximum Likelihood")~\newline
\verb~>   }~\newline
\verb~>   out$sig2 <- x$sig2~\newline
\verb~>   out$rho  <- x$rho~\newline
\verb~>   class(out) <- "trim.summary"~\newline
\verb~>   out~\newline
\verb~> }~\par

\verb~> print.trim.summary <- function(x) {~\newline
\verb~>   printf("\nEstimation method = %s\n", x$est.method)~\newline
\verb~>   if (is.finite(x$sig2)) printf("  Estimated Overdispersion     = %f\n", x$sig2)~\newline
\verb~>   if (is.finite(x$rho))  printf("  Estimated Serial Correlation = %f\n", x$rho)~\newline
\verb~> }~\par




\subsection{Coefficients}\par



\subsubsection{Extract}\par

\verb~> coef.trim <- function(x, which=c("additive","multiplicative","both")) {~\newline
\verb~>   stopifnot(class(x)=="trim")~\newline
\verb~>   which <- match.arg(which)~\par
Last 4 columns contain the additive and multiplicative parameters.
Select the appropriate subset from these, and all columns before these 4.\par
\verb~>   n = ncol(x$coefficients)~\newline
\verb~>   stopifnot(n>=4)~\newline
\verb~>   if (which=="additive") {~\newline
\verb~>     cols <- 1:(n-2)~\newline
\verb~>   } else if (which=="multiplicative") {~\newline
\verb~>     cols <- c(1:(n-4), (n-1):n)~\newline
\verb~>   } else if (which=="both") {~\newline
\verb~>     cols = 1:n~\newline
\verb~>   } else stop(sprintf("Invalid options which=%s", which))~\newline
\verb~>   out <- x$coefficients[cols]  ~{\sffamily\# ??? does not return correct function output}\newline
\verb~>   out~\newline
\verb~> }~\par



\subsection{Time totals}\par



\subsubsection{Extract}\par

\verb~> totals <- function(x, which=c("imputed","model","both")) {~\newline
\verb~>   stopifnot(class(x)=="trim")~\par

Select output columns from the pre-computed time totals\par
\verb~>   which <- match.arg(which)~\newline
\verb~>   if (which=="model") {~\newline
\verb~>     totals = x$time.totals[c(1,2,3)]~\newline
\verb~>   } else if (which=="imputed") {~\newline
\verb~>     totals = x$time.totals[c(1,4,5)]~\newline
\verb~>   } else if (which=="both") {~\newline
\verb~>     totals <- x$time.totals~\newline
\verb~>   } else stop(sprintf("Invalid options which=%s", which))~\par

\verb~>   totals~\newline
\verb~> }~\par





\subsection{Reparameterisation of Model 3}\par



\subsubsection{extract}\par

\verb~> linear <- function(x) {~\newline
\verb~>   stopifnot(class(x)=="trim")~\newline
\verb~>   stopifnot(x$model==3)~\par

\verb~>   structure(list(trend=x$linear.trend, dev=x$deviations), class="trim.linear")~\newline
\verb~> }~\par



\subsubsection{print}\par

\verb~> print.trim.linear <- function(x) {~\newline
\verb~>   stopifnot(class(x)=="trim.linear")~\par

\verb~>   printf("Linear Trend + Deviations for Each Time\n")~\newline
\verb~>   print(x$trend, row.names=TRUE)~\newline
\verb~>   printf("\n")~\newline
\verb~>   print(x$dev, row.names=FALSE)~\newline
\verb~> }~\par





\subsection{Plotting}\par


Plotting of data is easier if we convert it to a data table.
Here is a function that does the conversion.\par
\verb~> mat2df <- function(m, src=NA) {~\newline
\verb~>   nsite <- nrow(m)~\newline
\verb~>   ntime <- ncol(m)~\newline
\verb~>   df <- data.frame(~\newline
\verb~>     Site  = factor(rep(1:nsite, times=ntime)),~\newline
\verb~>     Time  = rep(1:ntime, each=nsite),~\newline
\verb~>     Count = as.vector(m)~\newline
\verb~>   )~\newline
\verb~>   df <- df[order(df$Site, df$Time), ]  ~{\sffamily\# Sort by site, then by time}\newline
\verb~>   if (!is.na(src)) df$Source=src  ~{\sffamily\# set optional data source}\newline
\verb~>   return(df)~\newline
\verb~> }~\par

\verb~> plot.trim <- function(x) {~\par
Prepare for plotting. First convert the estimations $\mu$ to a data frame.
Because these estimations are for specific time points, we call it the `discrete' model.\par
\verb~>   Discrete <- rbind(~\newline
\verb~>     mat2df(x$data,  "Data"),~\newline
\verb~>     mat2df(x$mu,    "Model")~\newline
\verb~>   )~\newline
\verb~>   Discrete <- subset(Discrete, is.finite(Count))~\par

Similarly create a data frame for estimations applied to continuous time (the 'continuous' model)\par
\verb~>   if (x$model!=3) {~\newline
\verb~>     ctime <- seq(1, x$ntime, length.out=100)~\newline
\verb~>     CModel <- data.frame()~\newline
\verb~>     for (i in 1:x$nsite) {~\newline
\verb~>       if (x$model==1) {~\newline
\verb~>         tmp <- data.frame(Site=i, Time=ctime, Count=exp(out$alpha[i]))~\newline
\verb~>       } else if (out$model==2) {~\newline
\verb~>         tmp <- data.frame(Site=i, Time=ctime, Count=exp(out$alpha[i] + out$beta*(ctime-1.0)))~\newline
\verb~>       }~\newline
\verb~>       CModel <- rbind(CModel, tmp)~\newline
\verb~>     }~\newline
\verb~>     CModel$Site <- factor(CModel$Site)~\newline
\verb~>   }~\par

Plot data and model (both discrete and continuous)\par
\verb~>   g <- ggplot(Discrete, aes(x=Time, y=Count, colour=Site)) + theme_bw()~\newline
\verb~>   g <- g + geom_point(aes(shape=Source), size=4)~\newline
\verb~>   g <- g + scale_shape_manual(values=c(1,20))~\newline
\verb~>   if (model!=3) g <- g + geom_path(data=CModel)~\newline
\verb~>   g <- g + scale_x_continuous(breaks=1:x$ntime)~\newline
\verb~>   g <- g + labs(shape="")~\newline
\verb~>   if (nchar(title)>0) g <- g + labs(title=title)~\par

Add the overall trend (based on the imputed)\par
\verb~>   intercept <- x$overall.slope$imp$coef[[1]][1]~\newline
\verb~>   slope     <- x$overall.slope$imp$coef[[1]][2]~\newline
\verb~>   t <- seq(1, x$ntime, length.out=100)~\newline
\verb~>   y <- exp(intercept + slope*t)~\newline
\verb~>   trend <- data.frame(Time=t, Count=y, Site=NA)~\newline
\verb~>   g <- g + geom_path(data=trend)~\par

\verb~>   print(g)~\par

\verb~> }~\par




\subsubsection{Plotting}\par


\verb~> plot_data_df <- function(df, title="") {~\newline
\verb~>   ntime <- max(df$Time)~\par
remove NA rows\par
\verb~>   df <- subset(df, is.finite(Count))~\newline
\verb~>   g <- ggplot(df, aes(x=Time,y=Count,colour=Site)) + theme_bw()~\newline
\verb~>   g <- g + geom_path(linetype="dashed")~\newline
\verb~>   g <- g + geom_point(size=4, shape=20)~\newline
\verb~>   g <- g + scale_x_continuous(breaks=1:ntime)~\newline
\verb~>   if (nchar(title)) g <- g + labs(title=title)~\newline
\verb~>   print(g)~\newline
\verb~> }~\par


\verb~> plot_data_mat <- function(m, ...) {~\newline
\verb~>   df <- mat2df(m)~\newline
\verb~>   plot_data_df(df, ...)~\newline
\verb~> }~\par


\title{TRIM code documentation}
\author{Patrick Bogaart}
\date{\today}\par



\section{Wald tests}\par



\subsection{Theory}\par

TRIM provides a number of tests for the significance of groups of parameters.
These so called Wald-tests are based on the estimated covariance matrix of the parameters,
and since this covariance matrix takes the overdispersion and serial
correlation into account (if specified), these tests are valid not only if
the counts are assumed to be independent Poisson observations but also if
$\sigma$ and/or $\rho$ are estimated.\par

The form of the Wald-statistic for testing simultaneously whether several parameters are different from zero is
\begin{equation}
  W = {\hat\theta}^T \left[\var{\hat\theta}\right]^{-1} \hat\theta
  \label{Wald-vector}
\end{equation}
with $\hat\theta$ a vector containing the parameter estimates to be tested,
and $\var{\hat\theta}$ the covariance matrix of $\hat\theta$.
For the univariate case (i.e., $\theta$ is a scalar),
Eqn~\eqref{Wald-vector} reduces to the univariate case
\begin{equation}
  W = {\hat\theta}^2 / \var{\hat\theta}
  \label{Wald-scalar}
\end{equation}\par

The following Wald-tests are performed by TRIM
\begin{itemize}
\item Test for the significance of the slope parameter (model 2).
\item Tests for the significance of changes in slope (model 2).
\item Test for the significance of the deviations from a linear trend (model 3).
\item Tests for the significance of the effect of each covariate (models 2 and 3).
\end{itemize}
The Wald-tests are asymptotically $\chi^2_{df}$ distributed,
with the number of degrees of freedom equal to the rank of the covariance matrix $\var{\hat\theta}$.
The hypothesis that the tested parameters are zero is rejected for large values of the test-statistic and small values of the associated significance probabilities (denoted by $p$),
so parameters are significantly different from zero if $p$ is smaller than some chosen significance level (customary choices are 0.01, 0.05 and 0.10).\par




\subsection{Computation}\par

\verb~> wald <- function(x) UseMethod("wald")~\par

\verb~> wald.trim <- function(z)~\newline
\verb~> {~\par
Collect TRIM output variables that we need here.\par
\verb~>   model  <- z$model~\newline
\verb~>   ntime  <- z$ntime~\newline
\verb~>   nbeta  <- length(z$beta)~\newline
\verb~>   nbeta0 <- z$nbeta0~\newline
\verb~>   covars <- z$covars~\newline
\verb~>   ncovar <- z$ncovar~\newline
\verb~>   nclass <- z$nclass~\newline
\verb~>   beta <- z$beta~\newline
\verb~>   var_beta <- z$var_beta~\par

\verb~>   if (model==3 && ncovar==0) {~\newline
\verb~>     gstar <- z$gstar~\newline
\verb~>     var_gstar <- z$var_gstar~\newline
\verb~>   }~\par

\verb~>   wald <- list()  ~{\sffamily\# Create empty output object}\par

Test for the significance of the slope parameter ---------------------------\par

This test applies to the case where model 2 is used without covariates or changepoints.
There thus is a single $\beta$ representing the trend for all sites and throughout the whole period,
and the univariate approach Eqn~\eqref{Wald-scalar} applies.\par
\verb~>   if (model==2 && nbeta==1 && ncovar==0) {~\newline
\verb~>     theta <- as.numeric(beta)~\newline
\verb~>     var_theta <- as.numeric(var_beta)~\newline
\verb~>     W  <- theta^2 / var_theta  ~{\sffamily\# Compute the Wald statistic by \eqref{Wald-scalar}}\newline
\verb~>     df <- 1  ~{\sffamily\# Degrees of freedom}\newline
\verb~>     p  <- 1 - pchisq(W, df=df)  ~{\sffamily\# $p$-value, based on $W$ being $\chi^2$ distributed.}\newline
\verb~>     wald$slope <- list(W=W, df=df, p=p)~\newline
\verb~>   }~\par

Tests for the significance of changes in slope -----------------------------\par

When model 2 is used with changepoints, $\beta$ is now a vector slope parameters,
and the Wald test is used to test if these slopes significantly change after a changepoint.
Thus, the Wald test is not applied to individual slope magnitudes $\beta_i$,
but on the \emph{change in} slope $\beta'_i$, where
$$ \beta'_i = \beta_i - \beta_{i-1} $$ and $$ \beta'_1 = \beta_1 $$
The vector $\beta'$ can be obtained from the linear tranformation
$ \beta' = A \beta$
where transformation matrix $A$ is a simple banded matrix structured as\par


$$ A = \begin{pmatrix*}[r]
   1 &  0 & 0 & \cdots \\
  -1 &  1 & 0 & \cdots \\
   0 & -1 & 1 & \cdots \\
   \vdots & \vdots & \vdots & \ddots
  \end{pmatrix*} $$
that is,
\begin{equation}
  A_{i,j} = \begin{cases}
     1 &\quad\text{for $i=j$},\\
    -1 &\quad\text{for $i=j+1$},\\
     0 &\quad\text{otherwise}.
  \end{cases}
\end{equation}\par
\verb~>   else if (model==2 && nbeta>1 && ncovar==0) {~\newline
\verb~>     A <- diag(nbeta)  ~{\sffamily\# Start with a diagonal matrix}\newline
\verb~>     idx <- row(A)==(col(A)+1)  ~{\sffamily\# The band just below the diagonal}\newline
\verb~>     A[idx] <- -1~\newline
\verb~>     dbeta <- A %*% beta~\par
The covariance matrix of $\beta'$, $V^{\beta'}$, can be obtained from $V^\beta$ by
applying the Taylor (???) delta (???) method
$$ V^{\beta'} = A V^\beta A^T $$
and the resulting diagonal elements can be taken as the variance of the corresponding $\beta'$:
$$ \var{\beta'_i} = V^{\beta'}_{i,i} $$\par
\verb~>     var_dbeta <- A %*% var_beta %*% t(A)~\par
Note that the Wald test is applied to each change point individually.\par
\verb~>     theta <- as.numeric(dbeta)~\newline
\verb~>     var_theta <- diag(var_dbeta)~\newline
\verb~>     W <- theta^2 / var_theta  ~{\sffamily\# Eqn~\eqref{Wald-scalar}}\newline
\verb~>     df <- 1  ~{\sffamily\# degrees of freedom}\newline
\verb~>     p  <- 1 - pchisq(W, df=df)  ~{\sffamily\# $p$-value, based on $W$ being $\chi^2$ distributed.}\newline
\verb~>     wald$dslope <- list(W=W, df=df, p=p)~\newline
\verb~>   }~\par

A variant is the same test for  multiple covariates\par
\verb~>   else if (model==2 && nbeta>1 && ncovar>0) {~\par
Again compute the transformation matrix\par
\verb~>     A <-diag(nbeta0)~\newline
\verb~>     idx <- row(A)==(col(A)+1)  ~{\sffamily\# band just below the diagonal}\newline
\verb~>     A[idx] <- -1~\par
Parameter vector $\beta$ and it's covariance matrix $V^\beta$ consists of `blocks'
representing either the baseline slopes $\beta_0$ or the impact of covariates $\beta_k$.
If we have $n$ changepoints, then these blocks are $n\times 1$ and $n\times n$ for
$\beta$ and $V^\beta$, respectively. First create an index for the first block.\par
\verb~>     nblock = sum(nclass-1)+1~\newline
\verb~>     stopifnot(nblock*nbeta0==nbeta)~\newline
\verb~>     idx0 = 1:nbeta0~\par
Again, $\beta'$ is easily computed from $\beta$, except that the transformation
$\beta' = A\beta$ is applied to each block\par
\verb~>     dbeta = matrix(0, nbeta, 1)~\newline
\verb~>     for (i in 1:nblock) {~\newline
\verb~>       idx = idx0 + nbeta0*(i-1)~\newline
\verb~>       dbeta[idx, ] <- A %*% beta[idx, ]~\newline
\verb~>     }~\par
idem for the covariance matrix\par
\verb~>     var_dbeta <- matrix(0, nbeta, nbeta)~\newline
\verb~>     for (i in 1:nblock) {~\newline
\verb~>       ridx = seq(to=i*nbeta0, len=nbeta0)  ~{\sffamily\# ((i-1)*nbeta0+1) : (i*nbeta0)}\newline
\verb~>       for (j in 1:nblock) {~\newline
\verb~>         cidx <- seq(to=j*nbeta0, len=nbeta0)~\newline
\verb~>         var_dbeta[ridx,cidx] <- A %*% var_beta[ridx,cidx] %*% t(A)~\newline
\verb~>       }~\newline
\verb~>     }~\par

Compute a Wald statistic for each changepoint\par
\verb~>     W = numeric(nbeta0)~\newline
\verb~>     for (b in 1:nbeta0) {~\newline
\verb~>       idx <- seq(from=b, by=nbeta0, len=nblock)~\newline
\verb~>       theta = dbeta[idx]~\newline
\verb~>       var_theta = var_dbeta[idx,idx]~\newline
\verb~>       W[b] = t(theta) %*% solve(var_theta) %*% theta~\newline
\verb~>     }~\newline
\verb~>     df <- nblock~\newline
\verb~>     p <- 1 - pchisq(W, df)~\newline
\verb~>     wald$dslope <- list(W=W, df=df, p=p)~\newline
\verb~>   }~\par


Test for the significance of the deviations from a linear trend ------------\par

For Model 3, we use the Wald test to test if the residuals around the overall
trend (i.e., the $\gamma_j^\ast$) significantly differ from 0.
For this case, the vectorized Wald equation~\eqref{Wald-vector} is used.
Note that this test is only performed when there are no covariates.\par
\verb~>   else if (model==3 && ncovar==0) {~\newline
\verb~>     J <- ntime~\newline
\verb~>     theta <- matrix(gstar)  ~{\sffamily\# Column vector of all $J$ $\gamma^\ast$.}\newline
\verb~>     var_theta <- var_gstar[-1,-1]  ~{\sffamily\# Covariance matrix; drop the $\beta^\ast$ terms.}\par

We now have $J$ equations, but due to the double contraints 2 of them are linear
dependent on the others. Let's confirm this:\par
\verb~>     eig <- eigen(var_theta)$values~\newline
\verb~>     stopifnot(sum(eig<1e-7)==2)~\par

Shrink $\theta$ and it's covariance matrix to remove the dependent equations.\par
\verb~>     theta <- theta[3:J]~\newline
\verb~>     var_theta <- var_theta[3:J, 3:J]~\par

\verb~>     W <- t(theta) %*% solve(var_theta) %*% theta  ~{\sffamily\# Eqn~\eqref{Wald-vector}}\newline
\verb~>     W <- as.numeric(W)  ~{\sffamily\# Convert from $1\times1$ matrix to proper atomic}\newline
\verb~>     df <- J-2  ~{\sffamily\# degrees of freedom}\newline
\verb~>     p  <- 1 - pchisq(W, df=df)  ~{\sffamily\# $p$-value, based on $W$ being $\chi^2$ distributed.}\par

\verb~>     wald$deviations <- list(W=W, df=df, p=p)~\newline
\verb~>   }~\newline
\verb~>   else if (model==3 && ncovar>0) {~\par
pass\par
\verb~>   } else stop("Can't happen")~\par

Tests for the significance of the effect of each covariate -----------------\par

As explained in ????, for both models 2 and 3, the covariate effects are modelled as additions $\beta_k$ to the
baseline slope parameters $\beta_0$ which represent the first class of all covariates.\par
\verb~>   if (ncovar>0) {~\newline
\verb~>     wald$covar <- data.frame(Covariate=names(covars), W=0, df=0, p=0)~\newline
\verb~>     size <- (nclass-1)*nbeta0  ~{\sffamily\# size of covariate block witin total $\beta$ vector}\newline
\verb~>     last <- cumsum(size)+nbeta0  ~{\sffamily\# last element of covariate block}\newline
\verb~>     first <- last - size + 1  ~{\sffamily\# first element}\newline
\verb~>     for (i in 1:length(covars)) {~\newline
\verb~>       idx <- first[i] : last[i]~\newline
\verb~>       theta <- matrix(beta[idx])~\newline
\verb~>       var_theta <- var_beta[idx, idx]~\newline
\verb~>       W = t(theta) %*% solve(var_theta) %*% theta~\newline
\verb~>       wald$covar$W[i]     <- W~\newline
\verb~>       wald$covar$df[i]    <- nbeta0~\newline
\verb~>       wald$covar$p[i]     <- 1 - pchisq(W, df=nbeta0)~\newline
\verb~>     }~\newline
\verb~>   }~\par

Output results in a list with type 'trim.wald' to enable specialized further processing.\par
\verb~>   class(wald) <- "trim.wald"~\newline
\verb~>   wald~\newline
\verb~> }~\par




\subsection{Printing}\par

A simple printing function is provided that mimics the output of TRIM for Windows.\par

\verb~> print.trim.wald <- function(w) {~\newline
\verb~>   stopifnot(class(w)=="trim.wald")~\par

\verb~>   if (!is.null(w$covar)) {~\newline
\verb~>     printf("Wald test for significance of covariates\n")~\newline
\verb~>     print(w$covar, row.names=FALSE)~\newline
\verb~>     printf("\n")~\newline
\verb~>   }~\par

\verb~>   if (!is.null(w$slope)) {~\newline
\verb~>     printf("Wald test for significance of slope parameter\n")~\newline
\verb~>     printf("  Wald = %.2f, df=%d, p=%f\n", w$slope$W, w$slope$df, w$slope$p)~\newline
\verb~>   } else if (!is.null(w$dslope)) {~\newline
\verb~>     printf("Wald test for significance of changes in slope\n")~\newline
\verb~>     df = data.frame(Changepoint = 1:length(w$dslope$W),~\newline
\verb~>                     Wald_test = w$dslope$W, df = w$dslope$df, p = w$dslope$p)~\newline
\verb~>     print(df, row.names=FALSE)~\newline
\verb~>   } else if (!is.null(w$deviations)) {~\newline
\verb~>     printf("Wald test for significance of deviations from linear trend\n")~\newline
\verb~>     printf("  Wald = %.2f, df=%d, p=%f\n", w$deviations$W, w$deviations$df, w$deviations$p)~\newline
\verb~>   }~\newline
\verb~> }~\par


\section{Indices}\par




\subsection{Internal workhorse function}\par

\verb~> .index <- function(tt, var_tt, b) {~\par

Time index $\tau_j$ is defined as time totals $\Mu$, normalized by the time total for the base
year, i.e.\,
$$ \tau_j = \Mu_j / \Mu_b $$
where $b\in[1\ldots J]$ indicates the base year.\par
\verb~>   tau <- tt / tt[b]~\par

Uncertainty is again quantified as a standard error $\sqrt{\var{\cdot}}$,
approximated using the delta method, now extended for the multivariate case:
\begin{equation}
  \var{\tau_j} = \var{f(\Mu_b,\Mu_j)} = d^T V(\Mu_b,\Mu_j) d \label{var_tau}
\end{equation}
where $d$ is a vector containing the partial derivatives of $f(\Mu_b,\Mu_j)$
\begin{equation}
  d = \begin{pmatrix} -\Mu_j \Mu_b^{-2} \\ \Mu_b^{-1} \end{pmatrix}
\end{equation}
and $V$ the covariance matrix of $\Mu_b$ and $\Mu_j$:
\begin{equation}
  V(\Mu_b,\Mu_j) = \begin{pmatrix*}[l]
    \var{\Mu_b} & \cov{\Mu_b, \Mu_j} \\
    \cov{\Mu_b, \Mu_j} & \var{\Mu_j}
  \end{pmatrix*}
\end{equation}
Note that for the base year $b$, where $\tau_b\equiv1$, Eqn~\eqref{var_tau} results in
$\var{\tau_b}=0$, which is also expected conceptually because $\tau_b$ is not an estimate but an exact and fixed result.\par
\verb~>   J <- length(tt)~\newline
\verb~>   var_tau <- numeric(J)~\newline
\verb~>   for (j in 1:J) {~\newline
\verb~>     d <- matrix(c(-tt[j] / tt[b]^2, 1/tt[b]))~\newline
\verb~>     V <- var_tt[c(b,j), c(b,j)]~\newline
\verb~>     var_tau[j] <- t(d) %*% V %*% d~\newline
\verb~>   }~\par

\verb~>   out = list(tau=tau, var_tau=var_tau)~\newline
\verb~> }~\par





\subsection{User interface}\par

\verb~> index <- function(trm, base=1, which=c("imputed","model","both")) {~\newline
\verb~>   stopifnot(class(trm)=="trim")~\par

Computation and output is user-configurable\par
\verb~>   which <- match.arg(which)~\newline
\verb~>   if (which=="model") {~\par
Call workhorse function to do the actual computation\par
\verb~>     mod <- .index(trm$tt_mod, trm$var_tt_mod, base)~\par
Store results in a data frame\par
\verb~>     out = data.frame(time  = 1:trm$ntime,~\newline
\verb~>                      model = mod$tau,~\newline
\verb~>                      se_mod = sqrt(mod$var_tau))~\newline
\verb~>   } else if (which=="imputed") {~\par
Idem, using the imputed time totals instead\par
\verb~>     imp <- .index(trm$tt_imp, trm$var_tt_imp, base)~\newline
\verb~>     out = data.frame(time    = 1:trm$ntime,~\newline
\verb~>                      imputed = imp$tau,~\newline
\verb~>                      se_imp  = sqrt(imp$var_tau))~\newline
\verb~>   } else if (which=="both") {~\par
Idem, using both modelled and imputed time totals.\par
\verb~>     mod <- .index(trm$tt_mod, trm$var_tt_mod, base)~\newline
\verb~>     imp <- .index(trm$tt_imp, trm$var_tt_imp, base)~\newline
\verb~>     out = data.frame(time    = 1:trm$ntime,~\newline
\verb~>                      model   = mod$tau,~\newline
\verb~>                      se_mod  = sqrt(mod$var_tau),~\newline
\verb~>                      imputed = imp$tau,~\newline
\verb~>                      se_imp  = sqrt(imp$var_tau))~\newline
\verb~>   } else stop("Can't happen")  ~{\sffamily\# because other cases are catched by match.arg()}\par

\verb~>   out~\newline
\verb~> }~\par




\section{Overall slope}\par

\verb~> overall <- function(x, which=c("imputed","model")) {~\newline
\verb~>   stopifnot(class(x)=="trim")~\newline
\verb~>   which = match.arg(which)~\par

extract vars from TRIM output\par
\verb~>   tt_mod <- z$tt_mod~\newline
\verb~>   tt_imp <- z$tt_imp~\newline
\verb~>   var_tt_mod <- z$var_tt_mod~\newline
\verb~>   var_tt_imp <- z$var_tt_imp~\newline
\verb~>   ntime <- z$ntime~\par

The overall slope is computed for both the modeled and the imputed $\Mu$'s.
So we define a function to do the actual work\par

\verb~>   .compute.overall.slope <- function(tt, var_tt) {~\newline
\verb~>     stopifnot(length(tt)==ntime)~\newline
\verb~>     J <- ntime~\par

Use Ordinary Least Squares (OLS) to estimate slope parameter $\beta$\par
\verb~>     X <- cbind(1, seq_len(ntime))  ~{\sffamily\# design matrix}\newline
\verb~>     y <- matrix(log(tt))~\newline
\verb~>     bhat <- solve(t(X) %*% X) %*% t(X) %*% y  ~{\sffamily\# OLS estimate of $b = (\alpha,\beta)^T$}\newline
\verb~>     yhat <- X %*% bhat~\par

Apply the sandwich method to take heteroskedasticity into account\par
\verb~>     dvtt <- 1/tt_mod  ~{\sffamily\# derivative of $\log{\Mu}$}\newline
\verb~>     Om <- diag(dvtt) %*% var_tt %*% diag(dvtt)  ~{\sffamily\# $\var{log{\Mu}}$}\newline
\verb~>     var_beta <- solve(t(X) %*% X) %*% t(X) %*% Om %*% X %*% solve(t(X) %*% X)~\newline
\verb~>     b_err <- sqrt(diag(var_beta))~\par

Compute the $p$-value, using the $t$-distribution\par
\verb~>     df <- ntime - 2~\newline
\verb~>     t_val <- bhat[2] / b_err[2]~\newline
\verb~>     p <- 2 * pt(abs(t_val), df, lower.tail=FALSE)~\par


Also compute effect size as relative change during the monitoring period.\par
\verb~>     effect <- abs(yhat[J] - yhat[1]) / yhat[1]~\par


Reverse-engineer the SSR (sum of squared residuals) from the standard error\par
\verb~>     j <- 1:J~\newline
\verb~>     D <- sum((j-mean(j))^2)~\newline
\verb~>     SSR <- b_err[2]^2 * D * (J-2)~\par

Export the results\par
\verb~>     df <- data.frame(~\newline
\verb~>       Additive       = bhat,~\newline
\verb~>       std.err.       = b_err,~\newline
\verb~>       Multiplicative = exp(bhat),~\newline
\verb~>       std.err.       = exp(bhat) * b_err,~\newline
\verb~>       row.names      = c("Intercept","Slope"),~\newline
\verb~>       check.names    = FALSE~\newline
\verb~>     )~\newline
\verb~>     list(coef=df,p=p, effect=effect, J=J, tt=tt, err=z$time.totals[[3]], SSR=SSR)~\newline
\verb~>   }~\par

\verb~>   if (which=="imputed") {~\newline
\verb~>     out = .compute.overall.slope(tt_imp, var_tt_imp)~\newline
\verb~>     out$src = "imputed"~\newline
\verb~>   } else if (which=="model") {~\newline
\verb~>     out = .compute.overall.slope(tt_mod, var_tt_mod)~\newline
\verb~>     out$src = "model"~\newline
\verb~>   } else stop("Can't happen")~\newline
\verb~>   structure(out, class="trim.overall")~\newline
\verb~> }~\par



\subsubsection{Extract}\par




\subsubsection{Print}\par

\verb~> print.trim.overall <- function(x) {~\newline
\verb~>   stopifnot(class(x)=="trim.overall")~\par

Compute 95\% confidence interval of multiplicative slope\par
\verb~>   bhat <- x$coef[[3]][2]  ~{\sffamily\# multiplicative trend (i.e., not log-transformed)}\newline
\verb~>   berr <- x$coef[[4]][2]  ~{\sffamily\# corresponding standard error}\newline
\verb~>   alpha <- 0.05~\newline
\verb~>   df <- x$J-2~\newline
\verb~>   tval <- qt((1-alpha/2), df)~\newline
\verb~>   blo <- bhat - tval * berr~\newline
\verb~>   bhi <- bhat + tval * berr~\par
Compute effect size\par
\verb~>   change <- bhat ^ (x$J-1) - 1~\par
Build an informative string\par
\verb~>   info <- sprintf("p=%f, conf.int (mul)=[%.f, %f], change=%.2f%%", x$p, blo, bhi, 100*change)~\newline
\verb~>   printf("Overall slope (%s): %s\n", x$src, info)~\newline
\verb~>   print(x$coef, row.names=TRUE)~\newline
\verb~> }~\par



\subsubsection{Plot}\par

\verb~> plot.trim.overall <- function(X, imputed=TRUE, ...) {~\newline
\verb~>   title <- attr(X, "title")~\par

\verb~>   J <- X$J~\par

Collect all data for plotting: time-totals\par
\verb~>   j <- 1:J~\newline
\verb~>   ydata <- X$tt~\par

error bars\par
\verb~>   y0 = ydata - X$err~\newline
\verb~>   y1 = ydata + X$err~\par

Trend line\par
\verb~>   a <- X$coef[[1]][1]  ~{\sffamily\# intercept}\newline
\verb~>   b <- X$coef[[1]][2]  ~{\sffamily\# slope}\newline
\verb~>   x <- seq(1, J, length.out=100)~\newline
\verb~>   ytrend <- exp(a + b*x)~\par

Confidence band\par
\verb~>   xconf <- c(x, rev(x))~\newline
\verb~>   alpha <- 0.05~\newline
\verb~>   df <- J - 2~\newline
\verb~>   t <- qt((1-alpha/2), df)~\newline
\verb~>   dx2 <- (x-mean(j))^2~\newline
\verb~>   sumdj2 <- sum((j-mean(j))^2)~\newline
\verb~>   dy <- t * sqrt((X$SSR/(J-2))*(1/J + dx2/sumdj2))~\newline
\verb~>   ylo <- exp(a + b*x - dy)~\newline
\verb~>   yhi <- exp(a + b*x + dy)~\newline
\verb~>   yconf <- c(ylo, rev(yhi))~\par

Compute the total range of all plot elements\par
\verb~>   xrange = range(x)~\newline
\verb~>   yrange = range(range(yconf), range(y0), range(y1))~\par

Now plot layer-by-layer\par
\verb~>   cbred <- rgb(228,26,28, maxColorValue = 255)~\newline
\verb~>   cbblue <- rgb(55,126,184, maxColorValue = 255)~\newline
\verb~>   plot(xrange, yrange, type='n', xlab="Time point", ylab="Count", main=title)~\newline
\verb~>   polygon(xconf, yconf, col=gray(0.9), lty=0)~\newline
\verb~>   lines(x, ytrend, col=cbred, lwd=3)~\newline
\verb~>   segments(j,y0, j,y1, lwd=3, col=gray(0.5))~\newline
\verb~>   points(j, ydata, col=cbblue, type='b', pch=16, lwd=3)~\par

\verb~> }~\end{document}
