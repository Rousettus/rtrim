\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{parskip}
\usepackage[margin=72pt]{geometry}
\begin{document}
\let\Sum=\sum
\newcommand{\diag}[1]{\operatorname{diag}(#1)}
\newcommand{\var}[1]{\operatorname{var}(#1)}
\newcommand{\cov}[1]{\operatorname{cov}(#1)}
\newcommand{\se}[1]{\operatorname{S.E.}(#1)}
\newcommand{\Mu}{{\mu_{+}}} 

\title{TRIM core}
\author{Patrick Bogaart}
\date{\today}
\section{Introduction}
This document describes the core TRIM function.\par

Define a convenience function for console output\par
\verb~> printf <- function(fmt,...) { cat(sprintf(fmt,...)) }~\par



\subsection{Interface}\par

The main TRIM function takes just two parameters:
commands, wrapped in a TCF data structure,
and a data set.\par
\verb~> trim <- function(tcf, dat) {~\newline
\verb~>   start = Sys.time()~\par



\subsubsection{Preparation}\par

Get job parameters, use defaults if necessary\par
\verb~>   file   <- tcf@file~\newline
\verb~>   model  <- tcf@model~\newline
\verb~>   title  <- ifelse(is.na(tcf@title), "<Untitled>", tcf@title)~\newline
\verb~>   weight <- ifelse(is.na(tcf@weight), FALSE, tcf@weight)~\newline
\verb~>   missing_code <- tcf@missing  ~{\sffamily Use any <0 if this is missing}\newline
\verb~>   serialcor    <- ifelse(is.na(tcf@serialcor), FALSE, tcf@serialcor)~\newline
\verb~>   overdisp     <- ifelse(is.na(tcf@overdisp),  FALSE, tcf@overdisp)~\par

\verb~>   if (model==2) {~\newline
\verb~>     changepoints <- tcf@changepoints~\newline
\verb~>   }~\par

Finalize input by discarding the TCF objects.
All info should have been extracted from it by now.\par
\verb~>   rm(tcf)~\par

Create observation matrix $f$.
Convert the data from a data frame representation to a matrix representation.
It's OK to have missing site/time combinations; these will automatically
translate to NA values.\par
\verb~>   nsite <- dat$nsite~\newline
\verb~>   ntime <- dat$ntime~\newline
\verb~>   f <- matrix(0, nsite, ntime)~\newline
\verb~>   rows <- as.integer(dat$df$site)  ~{\sffamily `site' is a factor, thus this results in $1\ldots I$.}\newline
\verb~>   cols <- as.integer(dat$df$time)  ~{\sffamily idem, $1 \ldots J$.}\newline
\verb~>   idx <- (cols-1)*nsite+rows  ~{\sffamily Create column-major linear index from row/column subscripts.}\newline
\verb~>   f[idx] <- dat$df$count  ~{\sffamily ... such that we can paste all data into the right positions}\par

We often need some specific subset of the data, e.g.\ all observations for site 3.
These are conveniently found by combining the following indices:\par
\verb~>   observed <- is.finite(f)  ~{\sffamily Flags observed (TRUE) / missing (FALSe) data}\newline
\verb~>   site <- as.vector(row(f))  ~{\sffamily Internal site identifiers are the row numbers of the original matrix.}\newline
\verb~>   time <- as.vector(col(f))  ~{\sffamily Idem for time points.}\newline
\verb~>   nobs <- rowSums(observed)  ~{\sffamily Number of actual observations per site}\par

For model 2, we do not allow for changepoints $<1$ or $\geq J$. At the same time,
a changepoint $1$ must be present\par
\verb~>   if (model==2) {~\newline
\verb~>     stopifnot(all(changepoints>=1))~\newline
\verb~>     stopifnot(all(changepoints<ntime))~\newline
\verb~>     stopifnot(all(diff(changepoints)>0))~\newline
\verb~>     if (changepoints[1]!=1) changepoints = c(1, changepoints)~\newline
\verb~>   }~\par

We make use of the generic model structure
$$ \log\mu = A\alpha + B\beta $$
where design matrices $A$ and $B$ both have $IJ$ rows.
For efficiency reasons the model estimation algorithm works on a per-site basis.
There is thus no need to store these full matrices. Instead, $B$ is constructed as a
smaller matrix that is valid for any site, and $A$ is not used at all.\par

Create matrix $B$, which is model-dependent.\par
\verb~>   if (model==2) {~\newline
\verb~>     ncp  <-  length(changepoints)~\newline
\verb~>     J <- ntime~\newline
\verb~>     B = matrix(0, J, ncp)~\newline
\verb~>     for (i in 1:ncp) {~\newline
\verb~>       cp1  <-  changepoints[i]~\newline
\verb~>       cp2  <-  ifelse(i<ncp, changepoints[i+1], J)~\newline
\verb~>       if (cp1>1) B[1:(cp1-1), i]  <-  0~\newline
\verb~>       B[cp1:cp2,i]  <-  0:(cp2-cp1)~\newline
\verb~>       if (cp2<J) B[(cp2+1):J,i]  <-  B[cp2,i]~\newline
\verb~>     }~\newline
\verb~>   } else if (model==3) {~\par
Model 3 in it's canonical form uses a single time parameter $\gamma$ per time step,
so design matrix $B$ is essentially a $J\times$J identity matrix.
Note, hoewever, that by definition $\gamma_1=0$, so effectively there are $J-1$ $\gamma$-values to consider.
As a consequence, the first column is deleted.\par
\verb~>     B <- diag(ntime)  ~{\sffamily Construct $J\times$J identity matrix}\newline
\verb~>     B <- B[ ,-1]  ~{\sffamily Remove first column3fff}\newline
\verb~>   }~\par



\subsubsection{Setup parameters and state variables}\par

Parameter $\alpha$ has a unique value for each site.\par
\verb~>   alpha <- matrix(0, nsite,1)  ~{\sffamily Store as column vector}\par

Parameter $\beta$ is model dependent.\par
\verb~>   if (model==2) {~\par
For model 2 we have one $\beta$ per change points\par
\verb~>     beta = matrix(0, length(changepoints), 1)~\newline
\verb~>   } else if (model==3) {~\par
For model 3, we have one $\beta$ per time $j>1$; all are initialized to 0 (no time effects)\par
\verb~>     beta <- matrix(0, ntime-1,1)  ~{\sffamily Store as column vector}\newline
\verb~>   }~\par

Variable $\mu$ holds the estimated counts.\par
\verb~>   mu <- matrix(0, nsite, ntime)~\par



\subsection{Model estimation.}\par

TRIM estimates the model parameters $\alpha$ and $\beta$ in an iterative fashion:
\begin{align}
\alpha_i^t &= \log z_i' f_i - \log z_i' \exp(B_i \beta^{t-1}) \label{alpha} \\
\mu^t &= \exp(A\alpha^t + B\beta^{t-1} - \log w) \label{mu} \\
\beta^t &= \beta^{t-1} - (i_b)^{-1} U_b^\ast \label{beta}
\end{align}
where the superscript $t$ refers to the iteration.\par

Derivative matrix $i_b$ is defined as
\begin{equation}
  -i_b = \sum_i B_i' \left(\Omega_i - \frac{1}{d_i}\Omega_i z_i z_i' \Omega_i\right) B_i \label{i_b}
\end{equation}
where
\begin{equation}
  \Omega_i = \diag{\mu_i} V_i^{-1} \diag{\mu_i} \label{Omega_i}
\end{equation}
with $V_i$ the covariance matrix for site $i$, and
\begin{equation}
  d_i = z_i' \Omega_i z_i \label{d_i}
\end{equation}\par

Covariance matrix $V_i$ is defined by
\begin{equation}
  V_i = \sigma^2 \sqrt{\diag{\mu}} R \sqrt{\diag{\mu}} \label{V1}
\end{equation}
where $\sigma^2$ is a dispersion parameters and $R$ is an (auto)correlation matrix.
Both of these two elements are optional.
If the counts are perfectly Possion distributed, $\sigma^2=1$,
and if autocorrelation is disabled (i.e.\ counts are independent),
Eqn~\eqref{V1} reduces to
\begin{equation}
  V_i = \sigma^2 \diag{\mu} \label{V2}
\end{equation}\par

Dispersion parameter $\sigma^2$ is estimated as
\begin{equation}
  \hat{\sigma}^2 = \frac{1}{n_f - n_\alpha - n_\beta} \sum_{i,j} r_{ij}^2
\end{equation}
where the $n$ terms are the number of observations, $\alpha$'s and $\beta$'s, respectively.
Summation is over the observed $i,j$ only.
and $r_{ij}$ are Pearson residuals given by
\begin{equation}
  r_{ij} = (f_{ij} - \mu_{ij}) / \sqrt{\mu_{ij}}
\end{equation}\par

Summarizing, estimation of $\alpha$ and $\beta$ involves the iterative computation of, in order,
$\alpha$, $\mu$, $r$, $\sigma^2$, $R$, $V$, $d$, $\Omega$, $i_b$, $U_b$ and $\beta$.\par

\verb~>   max_iter <- 200  ~{\sffamily Define a maximum number of iterations to detect failure to converge}\newline
\verb~>   chi2 = 1000;~\newline
\verb~>   lik  = 1000;~\newline
\verb~>   dryruns = 3~\newline
\verb~>   stepsize       = 1~\newline
\verb~>   for (iter in 1:max_iter) {~\par

Remember current parameter values to trace convergence\par
\verb~>     old_par <- c(as.vector(alpha), as.vector(beta))~\newline
\verb~>     old_cnt <- as.vector(mu)~\newline
\verb~>     old_lik <- lik~\par



\subsubsection{Update site-parameters $\alpha$}
Update $\alpha_i$ using \eqref{alpha}:
$$ \alpha_i^t = \log z_i' f_i - \log z_i' \exp(B_i \beta^{t-1}) $$
where it is noted that for any vector $v$, $z' v$ is equivalent to the sum of the elements of $v$\par
\verb~>     for (i in 1:nsite) {~\newline
\verb~>       fi <- f[site==i & observed==TRUE]  ~{\sffamily vector}\newline
\verb~>       Bi <- B[observed[site==i], , drop=FALSE]~\newline
\verb~>       alpha[i] <- log(sum(fi)) - log(sum(exp(Bi %*% beta)))~\newline
\verb~>     }~\par



\subsubsection{Update count estimates $\mu$}
Update $\mu$ using Eqn~\eqref{mu}
$$ \mu^t = \exp(A\alpha^t + B\beta^{t-1} - \log w) $$
where it is noted that we do not use matrix $A$. Instead, the site-specific
parameters $\alpha_i$ are used directly:
$$ \mu_i^t = \exp(\alpha_i^t + B\beta^{t-1} - \log w) $$\par
\verb~>     for (i in 1:nsite) {~\newline
\verb~>       mu[i, ] <- exp(alpha[i] + B %*% beta)~\newline
\verb~>     }~\par



\subsubsection{Pearson residuals}
Compute Pearson residuals using
\begin{equation}
  r_{ij} = (f_{ij} - \mu_{ij}) / \sqrt{\mu_{ij}}
\end{equation}\par
\verb~>     r <- matrix(0,nsite,ntime)  ~{\sffamily Use 0 instead of NA for missing cases to increase performance}\newline
\verb~>     for (i in 1:nsite) for (j in 1:ntime) if (observed[i,j]) {~\newline
\verb~>       r[i,j] <- (f[i,j]-mu[i,j]) / sqrt(mu[i,j])~\newline
\verb~>     }~\par



\subsubsection{(Over)dispersion}
Estimate the dispersion parameter $\sigma^2$ from
\begin{equation}
  \hat{\sigma}^2 = \frac{1}{n_f - n_\alpha - n_\beta} \sum_{i,j} r_{ij}^2
\end{equation}
where the $n$ terms are the number of observations, $\alpha$'s and $\beta$'s, respectively.
Summation is over the observed $i,j$ only.
Note that the dispersion parameter is only computed when asked for.\par
\verb~>     if (iter>dryruns && overdisp) {~\newline
\verb~>       df <- sum(nobs) - length(alpha) - length(beta)  ~{\sffamily degrees of freedom}\newline
\verb~>       sig2 <- sum(r^2) / df~\newline
\verb~>     } else {~\newline
\verb~>       sig2 <- 1.0~\newline
\verb~>     }~\par



\subsubsection{Autocorrelation}
The (optional) autocorrelation structure for any site $i$ is stored in $n_i\times n_i$ matrix $R_i$.
In case there are no missing values, $n_i=J$, and the `full' or `generic' autocorrelation matrix $R$ is expressed
as
\begin{equation}
R = \begin{pmatrix}
  1          & \rho       & \rho^2     & \cdots & \rho^{J-1} \\
  \rho       & 1          & \rho       & \cdots & \rho^{J-2} \\
  \vdots     & \vdots     & \vdots     & \ddots & \vdots     \\
  \rho^{J-1} & \rho^{J-2} & \rho^{J-3} & \cdots & 1
  \end{pmatrix}
\end{equation}
where $\rho$ is the lag-1 autocorrelation, estimated as
\begin{equation}
  \hat{\rho} = \frac{1}{n_{i,j,j+1}\hat{\sigma}^2} \left(\Sum_i^I\Sum_j^{J-1} r_{i,j}r_{i,j+1}) \right)
\end{equation}
where the summation is over observed pairs $i,j$--$i,j+1$, and $n_{i,j,j+1}$ is the number of pairs involved.
Again, both $\rho$ and $R$ are computes $\rho$ in a stepwise per-site fashion.
Also, site-specific autocorrelation matrices $R_i$ are formed by removing the rows and columns from $R$
corresponding with missing observations.\par

First estimate $\rho$\par
\verb~>     if (iter>dryruns && serialcor) {~\newline
\verb~>       rho   <-  0.0~\newline
\verb~>       count <-  0~\newline
\verb~>       for (i in 1:nsite) for (j in 1:(ntime-1)) {~\newline
\verb~>         if (observed[i,j] && observed[i,j+1]) {  ~{\sffamily short-circuit AND intended}\newline
\verb~>           rho <- rho + r[i,j] * r[i,j+1]~\newline
\verb~>           count <- count+1~\newline
\verb~>         }~\newline
\verb~>       }~\newline
\verb~>       rho <- rho / (count * sig2)~\newline
\verb~>     } else rho = 0.0~\par

Then build the `generic' (site-independent) matrix $R$:\par
\verb~>     if (iter>dryruns && serialcor) {~\newline
\verb~>       Rg <- rho ^ abs(row(diag(ntime)) - col(diag(ntime)))~\newline
\verb~>     }~\par



\subsubsection{Update $V$, $i_b$ and $U_b$.}
Information matrix $i_b$ Scorematrix $U_b^\ast$ are both dependent on site-specific matrices
$V_i$ and $\Omega_i$, which we compute on the fly along with site contributions to $i_b$ and $U_b$.\par

\verb~>     i_b <- 0~\newline
\verb~>     U_b <- 0~\newline
\verb~>     Omega <- vector("list", nsite)  ~{\sffamily store $\Omega_i$'s for later use}\newline
\verb~>     for (i in 1:nsite) {~\newline
\verb~>       mu_i <- mu[site==i & observed]~\newline
\verb~>       f_i  <- f[site==i & observed]~\newline
\verb~>       d_mu_i <- diag(mu_i, length(mu_i))  ~{\sffamily Length argument guarantees diag creation}\newline
\verb~>       if (iter>dryruns && serialcor) {~\newline
\verb~>         idx <- which(observed[i, ])~\newline
\verb~>         R_i <- Rg[idx,idx]~\newline
\verb~>         V_i <- sig2 * sqrt(d_mu_i) %*% R_i %*% sqrt(d_mu_i)~\newline
\verb~>       } else {~\newline
\verb~>         V_i <- sig2 * d_mu_i~\newline
\verb~>       }~\newline
\verb~>       V_inv <- solve(V_i)~\newline
\verb~>       Omega[[i]] <- d_mu_i %*% V_inv %*% d_mu_i~\newline
\verb~>       z <- matrix(1, nobs[i], 1)~\newline
\verb~>       d_i <- as.numeric(t(z) %*% Omega[[i]] %*% z)~\newline
\verb~>       #d_i <- sum(Omega[[i]])  ~{\sffamily equivalent to $z' \Omega_i z$.}\newline
\verb~>       B_i <- B[observed[site==i], ,drop=FALSE]~\newline
\verb~>       term <- t(B_i) %*% (Omega[[i]] - (Omega[[i]] %*% z %*% t(z) %*% Omega[[i]]) / d_i) %*% B_i~\newline
\verb~>       i_b <- i_b - term~\newline
\verb~>       U_b <- U_b + t(B_i) %*% d_mu_i %*% V_inv %*% (f_i - mu_i)~\newline
\verb~>     }~\par



\subsubsection{Check for convergence.}
The iterative estimation of $\alpha$ and $\beta$ is finished if convergence is reached.
This measured by the \emph{change in} $\alpha, \beta$.
Iteration stops if this change drops below a certain threshold.
Note that we check for convergence BEFORE we update beta...\par

\verb~>     lik <- 2*sum(f*log(f/mu), na.rm=TRUE)~\par

First compute the change in parameter values\par
\verb~>     new_par <- c(as.vector(alpha), as.vector(beta))~\newline
\verb~>     new_cnt <- as.vector(mu)~\newline
\verb~>     new_lik <- lik~\newline
\verb~>     max_par_change <- max(abs(new_par - old_par))  ~{\sffamily use the maximub absolute change}\newline
\verb~>     max_cnt_change <- max(abs(new_cnt - old_cnt))  ~{\sffamily use the maximub absolute change}\newline
\verb~>     max_lik_change <- max(abs(new_lik - old_lik))  ~{\sffamily use the maximub absolute change}\newline
\verb~>     rel_lik_change <- 100*abs(old_lik - new_lik)/abs(old_lik)~\par

Write out progress, and some more info\par
\verb~>     cat(sprintf("Iteration %2d;", iter))~\newline
\verb~>     cat(printf(" lik=%3f", lik))~\newline
\verb~>     if (overdisp)  cat(sprintf(" sig^2=%f", sig2))~\newline
\verb~>     if (serialcor) cat(sprintf(" rho=%f;", rho))~\newline
\verb~>     cat(sprintf(" Max change: %10g %10g %10g", max_par_change, max_cnt_change, max_lik_change))~\par

\verb~>     cat("\n")~\par

Exit loop when convergence has been reached\par
\verb~>     conv_par = max_par_change < 1e-7~\newline
\verb~>     conv_cnt = max_cnt_change < 1e-7~\newline
\verb~>     conv_lik = max_lik_change < 1e-7~\newline
\verb~>     if (conv_par) cat("Convergence reached (in parameters).\n")~\newline
\verb~>     if (conv_cnt) cat("Convergence reached (in counts).\n")~\newline
\verb~>     if (conv_lik) cat("Convergence reached (in likelihood).\n")~\newline
\verb~>     if (conv_par && conv_cnt && conv_lik) break~\par



\subsubsection{Update $\beta$}
Finally, we can update $\beta$ using \eqref{beta}:
$$ \beta^t &= \beta^{t-1} - (i_b)^{-1} U_b^\ast $$\par
\verb~>     dbeta  <-  - solve(i_b) %*% U_b~\newline
\verb~>     beta <- beta + stepsize*dbeta~\newline
\verb~>   }~\par
If we reach the preset maximum number of iterations, we clearly have not reached
convergence.\par
\verb~>   if (iter==max_iter) stop("No convergence reached.")~\par

Run the final model\par
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     mu[i, ] <- exp(alpha[i] + B %*% beta)~\newline
\verb~>   }~\par



\subsection{Imputation}
The imputation process itself is trivial: just replace all missing observations
$f_{i,j}$ by the model-based estimates $\mu_{i,j}$.\par
\verb~>   imputed <- ifelse(observed, f, mu)~\par




\subsection{Output and postprocessing}\par

Measured, modelled and imputed count data are stored in a TRIM output object,
together with parameter values and other usefull information.\par

\verb~>   z <- list(title=title, data=f, nsite=nsite, ntime=ntime,~\newline
\verb~>             model=model, mu=mu, imputed=imputed, alpha=alpha, beta=beta)~\newline
\verb~>   class(z) <- "trim"~\par

Several kinds of statistics can now be computed, and added to this output object.\par



\subsubsection{Overdispersion and Autocorrelation}\par

\verb~>   z$sig2 <- ifelse(overdisp, sig2, NA)~\newline
\verb~>   z$rho  <- ifelse(serialcor, rho,  NA)~\par



\subsubsection{Coefficients and uncertainty}\par

\verb~>   if (model==2) {~\newline
\verb~>     beta     <- as.vector(beta)~\newline
\verb~>     var_beta <- -solve(i_b)~\newline
\verb~>     se_beta  <- sqrt(diag(var_beta))~\par

Again, results are stored in the TRIM object\par
\verb~>     z$coefficients <- data.frame(~\newline
\verb~>       Additive      = beta,~\newline
\verb~>       std.err.      = se_beta,~\newline
\verb~>       Mutiplicative = exp(beta),~\newline
\verb~>       std.err.      = exp(beta) * se_beta,~\newline
\verb~>       check.names   = FALSE  ~{\sffamily to allow for 2 "std.err." columns}\newline
\verb~>     )~\newline
\verb~>     row.names(z$coefficients) <- "Slope"~\newline
\verb~>   }~\par

\verb~>   if (model==3) {~\par
Model coefficients are output in two types; as additive parameters:
$$ \log\mu_{ij} = \alpha_i + \gamma_j $$
and as multiplicative parameters:
$$ \mu_{ij} = a_i g_j $$
where $a_i=e^{\alpha_i}$ and $g_j = e^{\gamma_j}$.\par
\verb~>     gamma     <-  matrix(c(0, as.vector(beta)))  ~{\sffamily Add $\gamma_1\equiv1$, and cast as column vector}\newline
\verb~>     g         <- exp(gamma)~\par

Parameter uncertainty is expressed as standard errors.
For the additive parameters $\gamma$, the variance is estimated as
$$ \var{\gamma} = (-i_b)^{-1} $$\par
\verb~>     var_gamma <-  -solve(i_b)~\par
Because $\gamma_1\equiv1$, it was not estimated, and as a results $j=1$ was not
incuded in $i_b$, nor in $\var{gamma}$ as computed above.
We correct this by adding the `missing' rows and columns.\par
\verb~>     var_gamma <- cbind(0, rbind(0, var_gamma))~\par
Finally, we compute the standard error as $\se{\gamma} = \sqrt{\diag{\var{\gamma}}}$\par
\verb~>     se_gamma  <-  sqrt(diag(var_gamma))~\par

The standard error of the multiplicative parameters $g_j$ is opproximated by
using the delta method, which is based on a Taylor expansion:
\begin{equation}
  \var{f(\theta)} = \left(f'(\theta)\right)^2 \var{\theta}
\end{equation}
which for $f(\theta)=e^\theta$ translates to
$$ \var{g} = \var{e^{\gamma}} = e^{2\gamma} \var{\gamma} $$
leading to
$$ \se{g} = e^{\gamma} \se{\gamma} = g \se{\gamma} $$\par
\verb~>     se_g <- g * se_gamma~\par

Again, results are stored in the TRIM object\par
\verb~>     z$coefficients <- data.frame(~\newline
\verb~>       Time          = 1:ntime,~\newline
\verb~>       Additive      = gamma,~\newline
\verb~>       std.err.      = se_gamma,~\newline
\verb~>       Mutiplicative = g,~\newline
\verb~>       std.err.      = g * se_gamma,~\newline
\verb~>       check.names   = FALSE  ~{\sffamily to allow for 2 "std.err." columns}\newline
\verb~>     )~\newline
\verb~>   }~\par



\subsubsection{Goodness-of-fit}\par

The goodness-of-fit of the model is assessed using three statistics:
Chi-squared, Likelihood Ratio and Aikaike Information Content.\par

The $\chi^2$ (Chi-square) statistic is given by
\begin{equation}
  \chi^2 = \sum_{ij}\frac{f_{i,j}-\mu_{i,j}}{\mu_{i,j}}
\end{equation}
where the summation is over the observed $i,j$'s only.
Significance is assessed by comparing against a $\chi^2$ distribution with
$df$ degrees of freedom, equal to the number of observations
minus the total number of parameters involved, i.e.\
$df = n_f - n_\alpha - n_\beta$.\par
\verb~>   chi2 <- sum((f-mu)^2/mu, na.rm=TRUE)~\newline
\verb~>   df   <- sum(observed) - length(alpha) - length(beta)~\newline
\verb~>   p    <-  1 - pchisq(chi2, df=df)~\par
Results are stored in the TRIM output object.\par
\verb~>   z$chi2 <- list(chi2=chi2, df=df, p=p)~\par

Similarly, the \emph{Likelihood ratio} (LR) is computed as
\begin{equation}
  \operatorname{LR} = 2\sum_{ij}f_{ij} \log\frac{f_{i,j}}{\mu_{i,j}}
\end{equation}
and again compared against a $\chi^2$ distribution.\par
\verb~>   LR <- 2 * sum(f * log(f / mu), na.rm=TRUE)~\newline
\verb~>   df <- sum(observed) - length(alpha) - length(beta)~\newline
\verb~>   p  <- 1 - pchisq(LR, df=df)~\newline
\verb~>   z$LR <- list(LR=LR, df=df, p=p)~\par

The Akaike Information Content (AIC) is related to the LR as:\par
\verb~>   AIC <- LR - 2*df~\newline
\verb~>   z$AIC <- AIC~\par



\subsubsection{Time Totals}\par

Recompute $i_b$ with final $\mu$'s\par
\verb~>   ib <- 0~\newline
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     mu_i <- mu[site==i & observed]~\newline
\verb~>     n_i <- length(mu_i)~\newline
\verb~>     d_mu_i <- diag(mu_i, n_i)  ~{\sffamily Length argument guarantees diag creation}\newline
\verb~>     OM <- Omega[[i]]~\newline
\verb~>     d_i <- sum(OM)  ~{\sffamily equivalent with z' Omega z, as in the TRIM manual}\newline
\verb~>     B_i <- B[observed[site==i], ,drop=FALSE]~\newline
\verb~>     om <- colSums(OM)~\newline
\verb~>     OMzzOM <- om %*% t(om)  ~{\sffamily equivalent with OM z z' OM, as in the TRIM manual}\newline
\verb~>     term <- t(B_i) %*% (OM - (OMzzOM) / d_i) %*% B_i~\newline
\verb~>     ib <- ib - term~\newline
\verb~>   }~\par

Matrices E and F take missings into account\par
\verb~>   E <- -ib~\par

\verb~>   nbeta <- length(beta)~\newline
\verb~>   F <- matrix(0, nsite, nbeta)~\newline
\verb~>   d <- numeric(nsite)~\newline
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     d[i] <- sum(Omega[[i]])~\newline
\verb~>     w_i <- colSums(Omega[[i]])~\newline
\verb~>     B_i <- B[observed[site==i], ,drop=FALSE]~\newline
\verb~>     F_i <- (t(w_i) %*% B_i) / d[i]~\newline
\verb~>     F[i, ] <- F_i~\newline
\verb~>   }~\par

Matrices G and H are for all mu's\par

\verb~>   GddG <- matrix(0, ntime,ntime)~\newline
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     for (j in 1:ntime) for (k in 1:ntime) {~\newline
\verb~>       GddG[j,k] <- GddG[j,k] + mu[i,j]*mu[i,k]/d[i]~\newline
\verb~>     }~\newline
\verb~>   }~\par

\verb~>   GF <- matrix(0, ntime, nbeta)~\newline
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     for (j in 1:ntime) for (k in 1:nbeta)  {~\newline
\verb~>       GF[j,k] <- GF[j,k] + mu[i,j] * F[i,k]~\newline
\verb~>     }~\newline
\verb~>   }~\par

\verb~>   H <- matrix(0, ntime, nbeta)~\newline
\verb~>   for (i in 1:nsite) {~\newline
\verb~>     for (k in 1:nbeta) for (j in 1:ntime) {~\newline
\verb~>       H[j,k]  <- H[j,k] + B[j,k] * mu[i,j]~\newline
\verb~>     }~\newline
\verb~>   }~\par

\verb~>   GFminH <- GF - H~\par

All building blocks are ready. Use them to compute the variance\par
\verb~>   var_tt_mod <- GddG + GFminH %*% solve(E) %*% t(GFminH)~\par


Time totals of the model, and it's standard error\par
\verb~>   tt_mod    <- colSums(mu)~\newline
\verb~>   se_tt_mod <- round(sqrt(diag(var_tt_mod)))~\par

\verb~>   tt_imp     <- colSums(imputed)~\newline
\verb~>   var_tt_imp = matrix(NA, ntime, ntime)~\newline
\verb~>   se_tt_imp <- round(sqrt(diag(var_tt_imp)))~\par

\verb~>   z$time.totals <- data.frame(~\newline
\verb~>     Time       = 1:ntime,~\newline
\verb~>     Model      = round(tt_mod),~\newline
\verb~>     std.err.   = se_tt_mod,~\newline
\verb~>     Imputed    = round(tt_imp),~\newline
\verb~>     std.err.   = se_tt_imp,~\newline
\verb~>     check.names = FALSE~\newline
\verb~>   )~\par



\subsubsection{Time indices}\par

Time index $\tau_j$ is defined as time totals, normalized by the time total for the base
year, i.e.\,
$$ \tau_j = \Mu_j / \Mu_1 $$.
Indices are computed for both the modelled and the imputed counts.\par
\verb~>   ti_mod <- tt_mod / tt_mod[1]~\newline
\verb~>   ti_imp <- tt_imp / tt_imp[1]~\par

Uncertainty is again quantified as a standard error $\sqrt{var{\cdot}}$,
approximated using the delta method, now extended for the multivariate case:
\begin{equation}
  \var{\tau_j} = \var{f(\Mu_1,\Mu_j)} = d^T V(\Mu_1,\Mu_j) d \label{var_tau}
\end{equation}
where $d$ is a vector containing the partial derivatives of $f(\Mu_1,\Mu_j)$
\begin{equation}
  d = \begin{pmatrix} -\Mu_j \Mu_1^{-2} \\ \Mu_1^{-1} \end{pmatrix}
\end{equation}
and $V$ the covariance matrix of $\Mu_1$ and $\Mu_j$:
\begin{equation}
  V(\Mu_1,\Mu_j) = \begin{pmatrix}
    \var{\Mu_1} & \cov{\Mu_1, \Mu_j} \\
    \cov{\Mu_1, \Mu_j} & \var{\Mu_j}
  \end{pmatrix}
\end{equation}
Note that for the base year, where $\tau_1\equiv1$, Eqn~\eqref{var_tau} results in
$\var{\tau_1}=0$, which is also expected conceptually because $\tau_1$ is not an estimate but an exact and fixed result.\par
\verb~>   var_ti_mod <- numeric(ntime)~\newline
\verb~>   for (j in 1:ntime) {~\newline
\verb~>     d <- matrix(c(-tt_mod[j] / tt_mod[1]^2, 1/tt_mod[1]))~\newline
\verb~>     V <- var_tt_mod[c(1,j), c(1,j)]~\newline
\verb~>     var_ti_mod[j] <- t(d) %*% V %*% d~\newline
\verb~>   }~\newline
\verb~>   se_ti_mod <- sqrt(var_ti_mod)~\par

Similarly for the Indices based on the imputed counts\par
\verb~>   se_ti_imp <- numeric(ntime)~\newline
\verb~>   for (j in 1:ntime) {~\newline
\verb~>     d <- matrix(c(-tt_imp[j]/tt_imp[1]^2, 1/tt_imp[1]))~\newline
\verb~>     V <- var_tt_imp[c(1,j), c(1,j)]~\newline
\verb~>     se_ti_imp[j] <- sqrt(t(d) %*% V %*% d)~\newline
\verb~>   }~\par

Store in TRIM output object\par
\verb~>   z$time.index <- data.frame(~\newline
\verb~>     Time     = 1:ntime,~\newline
\verb~>     Model    = ti_mod,~\newline
\verb~>     std.err. = se_ti_mod,~\newline
\verb~>     Imputed  = ti_imp,~\newline
\verb~>     std.err. = se_ti_imp,~\newline
\verb~>     check.names = FALSE~\newline
\verb~>   )~\par



\subsubsection{Reparameterisation of Model 3}\par

Here we consider the reparameterization of the time-effects model in terms of a model with a linear trend and deviations from this linear trend for each time point.
The time-effects model is given by
\begin{equation}
  \log\mu_{ij}=\alpha_i+\gamma_j,
\end{equation}
with $\gamma_j$ the effect for time $j$ on the log-expected counts and $\gamma_1=0$. This reparameterization can be expressed as
\begin{equation}
  \log\mu_{ij}=\alpha^*_i+\beta^*d_j+\gamma^*_j,
\end{equation}
with $d_j=j-\bar{j}$ and $\bar{j}$ the mean of the integers $j$ representing
the time points.
The parameter $\alpha^*_i$ is the intercept and the parameter $\beta^*$ is
the slope of the least squares regression line through the $J$ log-expected
time counts in site $i$ and  $\gamma^*_j$ can be seen as the residuals of this
linear fit.
From regression theory we have that the `residuals'"'  $\gamma^*_j$ sum to zero
and are orthogonal to the explanatory variable, i.e.
\begin{equation}
  \sum_j\gamma^*_j = 0 \quad \text{and} \quad \sum_jd_j\gamma^*_j = 0. \label{constraints}
\end{equation}
Using these constraints we obtain the equations:
\begin{gather}
  \log\mu_{ij}           = \alpha^*_i+\beta^*d_j+\gamma^*_j=\alpha_i+\gamma_j  \label{repar1}\\
  \sum_j \log\mu_{ij}    = J\alpha^*_j = J\alpha_i+\sum_j\gamma_j \label{repar2}\\
  \sum_j d_j\log\mu_{ij} = \beta^*\sum_jd^2_j = \sum_jd_j\gamma_j \label{repar3},
\end{gather}
where \eqref{repar1} is the re-parameterization equation itself and \eqref{repar2}
and \eqref{repar3} are obtained by using the constraints~\eqref{constraints}\par

From \eqref{repar2} we have that $\alpha^*_i=\alpha_i+\frac{1}{J}\sum_j\gamma_j$.
Now, by using the equations \eqref{repar1} thru \eqref{repar3} and defining
$D=\sum_jd^2_j$, we can express the parameters $\beta^*$ and $\gamma^*$ as
functions of the parameters $\gamma$ as follows:
\begin{align}
  \label{betaster}
  \beta^* &=\frac{1}{D}\sum_jd_j\gamma_j,\\ \nonumber
  \label{gammaster}
  \gamma^*_j &= \alpha_i+\gamma_j-\alpha^*_i-\beta^*d_j  \quad (\text{using (5)})\\ \nonumber
  &=\alpha_i-\left( \alpha_i+\frac{1}{J}\sum_j\gamma_j\right) +\gamma_j-d_j\frac{1}{D}\sum_jd_j\gamma_j \\
  &=\gamma_j-\frac{1}{J}\sum_j\gamma_j-d_j\frac{1}{D}\sum_jd_j\gamma_j.
\end{align}
Since $\beta^*$ and $\gamma^*_j$ are linear functions of the parameters $\gamma_j$
they can be expressed in matrix notation by
\begin{equation}
  \left ( \begin{array} {c}
         \beta^* \\
         \boldsymbol{\gamma}^*
  \end{array} \right ) = \mathbf{T}\boldsymbol{\gamma},
\end{equation}
with $\boldsymbol{\gamma}^*=(\gamma^*_1,\ldots,\gamma^*_J)^T$,
$\boldsymbol{\gamma}=(\gamma_1,\ldots,\gamma_J)^T$ and $\mathbf{T}$
the $(J+1) \times J$ transformation matrix that transforms
$\boldsymbol{\gamma}$ to  $\left (\beta^*,(\boldsymbol{\gamma}^*)^T\right)^T$.
From \eqref{betaster} and \eqref{gammaster} it follows that the elements of
$\mathbf{T}$ are given by:
\begin{align}
  \label{matrixT} \nonumber
  &\mathbf{T}_{(1,j)}=\frac{d_j}{D} &\quad (i=1,j=1,\ldots,J)\\ \nonumber
  &\mathbf{T}_{(i,j)}=1-\frac{1}{J}-\frac{1}{D}d_{i-1}d_j &\quad(i=2,\ldots,J+1,j=1,\ldots,J,i-1=j)\\ \nonumber
  &\mathbf{T}_{(i,j)}=-\frac{1}{J}-\frac{1}{D}d_{i-1}d_j &\quad(i=2,\ldots,J+1,j=1,\ldots,J,i-1 \neq j)
\end{align}\par

\verb~>   if (model==3) {~\par

\verb~>     TT <- matrix(0, ntime+1, ntime)~\newline
\verb~>     J <- ntime~\newline
\verb~>     j <- 1:J; d <- j - mean(j)  ~{\sffamily i.e, $ d_j = j-\frac{1}{J}\sum_j j$}\newline
\verb~>     D <- sum(d^2)  ~{\sffamily i.e., $ D = \sum_j d_j^2$}\newline
\verb~>     TT[1, ] <- d / D~\newline
\verb~>     for (i in 2:(J+1)) for (j in 1:J) {~\newline
\verb~>       if (i-1 == j) {~\newline
\verb~>         TT[i,j] <- 1 - (1/J) - d[i-1]*d[j]/D~\newline
\verb~>       } else {~\newline
\verb~>         TT[i,j] <-   - (1/J) - d[i-1]*d[j]/D~\newline
\verb~>       }~\newline
\verb~>     }~\par

\verb~>     xstar <- TT %*% gamma~\newline
\verb~>     bstar <- xstar[1]~\newline
\verb~>     gstar <- xstar[2:(J+1)]~\par

The covariance matrix of the transformed parameter vector can now be obtained
from the covariance matrix $\mathbf{T}\boldsymbol{\gamma}$ of $\boldsymbol{\gamma}$ as
\begin{equation}
  V\left( \begin{array} {c} \beta^* \\\boldsymbol{\gamma}^* \end{array} \right)
  = \mathbf{T}V(\boldsymbol{\gamma})\mathbf{T}^T
\end{equation}\par

\verb~>     var_xstar <- TT %*% var_gamma %*% t(TT)~\newline
\verb~>     var_bstar <- as.numeric(var_xstar[1,1])~\newline
\verb~>     var_gstar <- var_xstar[-1,-1]~\newline
\verb~>     se_bstar  <- sqrt(var_bstar)~\newline
\verb~>     se_gstar  <- sqrt(diag(var_gstar))~\par

\verb~>     z$linear.trend <- data.frame(~\newline
\verb~>       Additive       = bstar,~\newline
\verb~>       std.err        = se_bstar,~\newline
\verb~>       Multiplicative = exp(bstar),~\newline
\verb~>       std.err.       = exp(bstar) * se_bstar,~\newline
\verb~>       row.names      = "Slope",~\newline
\verb~>       check.names    = FALSE)~\par

Deviations from the linear trend\par
\verb~>     z$deviations <- data.frame(~\newline
\verb~>       Time       = 1:ntime,~\newline
\verb~>       Additive       = gstar,~\newline
\verb~>       std.err.       = se_gstar,~\newline
\verb~>       Multiplicative = exp(gstar),~\newline
\verb~>       std.err.       = exp(gstar) * se_gstar,~\newline
\verb~>       check.names = FALSE~\newline
\verb~>     )~\par

\verb~>   }~\par


\subsubsection{Wald test}\par

\verb~>   if (model==2) {~\newline
\verb~>     theta = beta[1]~\newline
\verb~>     var_theta = var_beta[1,1]~\par

\verb~>     W <- t(theta) %*% solve(var_theta) %*% theta  ~{\sffamily Compute the Wald statistic}\newline
\verb~>     W <- as.numeric(W)  ~{\sffamily Convert from $1\times1$ matrix to proper atomic}\newline
\verb~>     df <- 1  ~{\sffamily degrees of freedom}\newline
\verb~>     p  <- 1 - pchisq(W, df=df)  ~{\sffamily $p$-value, based on $W$ being $\chi^2$ distributed.}\par

\verb~>     z$wald <- list(model=model, W=W, df=df, p=p)}~\par

For Model 3, we use the Wald test to test if the residuals around the overall
trend (i.e., the $\gamma_j^\ast$) significantly differ from 0.
The Wald statistic used for this is defined as
\begin{equation}
  W = \theta^T \left(\var{\theta}\right)^{-1} \theta
\end{equation}\par

\verb~>   if (model==3) {~\newline
\verb~>     theta <- matrix(gstar)  ~{\sffamily Column vector of all $J$ $\gamma^\ast$.}\newline
\verb~>     var_theta <- var_gstar  ~{\sffamily Covariance matrix; drop the $\beta^\ast$ terms.}\par

We now have $J$ equations, but due to the double contraints 2 of them are linear
dependent on the others. Let's confirm this:.\par
\verb~>     eig <- eigen(var_theta)$values~\newline
\verb~>     stopifnot(sum(eig<1e-7)==2)~\par

Shrink $\theta$ and it's covariance matrix to remove the dependent equations.\par
\verb~>     theta <- theta[3:J]~\newline
\verb~>     var_theta <- var_theta[3:J, 3:J]~\par

\verb~>     W <- t(theta) %*% solve(var_theta) %*% theta  ~{\sffamily Compute the Wald statistic}\newline
\verb~>     W <- as.numeric(W)  ~{\sffamily Convert from $1\times1$ matrix to proper atomic}\newline
\verb~>     df <- J-2  ~{\sffamily degrees of freedom}\newline
\verb~>     p  <- 1 - pchisq(W, df=df)  ~{\sffamily $p$-value, based on $W$ being $\chi^2$ distributed.}\par

\verb~>     z$wald <- list(model=model, W=W, df=df, p=p)~\par

\verb~>   }~\par


\subsubsection{Overall slope}\par

The overall slope is computed for both the modeled and the imputed $\Mu$'s.
So we define a function to do the actual work\par
\verb~>   .compute.overall.slope <- function(tt, var_tt) {~\par
Use Ordinary Least Squares (OLS) to estimate slope parameter $\beta$\par
\verb~>     X <- cbind(1, seq_len(ntime))  ~{\sffamily design matrix}\newline
\verb~>     y <- matrix(log(tt))~\newline
\verb~>     bhat <- solve(t(X) %*% X) %*% t(X) %*% y  ~{\sffamily OLS estimate of $b = (\alpha,\beta)^T$}\newline
\verb~>     yhat <- X %*% bhat~\par

Apply the sandwich method to take heteroskedasticity into account\par
\verb~>     dvtt <- 1/tt_mod  ~{\sffamily derivative of $\log{\Mu}$}\newline
\verb~>     Om <- diag(dvtt) %*% var_tt %*% diag(dvtt)  ~{\sffamily $\var{log{\Mu}}$}\newline
\verb~>     var_beta <- solve(t(X) %*% X) %*% t(X) %*% Om %*% X %*% solve(t(X) %*% X)~\newline
\verb~>     b_err <- sqrt(diag(var_beta))~\par

Compute the $p$-value, using the $t$-distribution\par
\verb~>     df <- ntime - 2~\newline
\verb~>     t_val <- bhat[2] / b_err[2]~\newline
\verb~>     p <- 2 * pt(abs(t_val), df, lower.tail=FALSE)~\par


Also compute effect size as relative change during the monitoring period.\par
\verb~>     effect <- abs(yhat[J] - yhat[1]) / yhat[1]~\par


Reverse-engineer the SSR (sum of squared residuals) from the standard error\par
\verb~>     j <- 1:J~\newline
\verb~>     D <- sum((j-mean(j))^2)~\newline
\verb~>     SSR <- b_err[2]^2 * D * (J-2)~\par

Export the results\par
\verb~>     df <- data.frame(~\newline
\verb~>       Additive       = bhat,~\newline
\verb~>       std.err.       = b_err,~\newline
\verb~>       Multiplicative = exp(bhat),~\newline
\verb~>       std.err.       = exp(bhat) * b_err,~\newline
\verb~>       row.names      = c("Intercept","Slope"),~\newline
\verb~>       check.names    = FALSE~\newline
\verb~>     )~\newline
\verb~>     list(coef=df,p=p, effect=effect, J=J, tt=tt, err=z$time.totals[[3]], SSR=SSR)~\newline
\verb~>   }~\par

Compute the overall trends for both the modelled and the imputed counts, and
store the results in the TRIM output\par
\verb~>   z$overall <- list()~\newline
\verb~>   z$overall$mod  <- .compute.overall.slope(tt_mod, var_tt_mod)~\newline
\verb~>   z$overall$imp  <- .compute.overall.slope(tt_imp, var_tt_imp)~\par



\subsection{Return results}\par

\verb~>   duration = Sys.time() - start~\newline
\verb~>   print(duration)~\par

The TRIM result is returned to the user\ldots\par
\verb~>   z~\newline
\verb~> }~\par
\ldots which ends the main TRIM function.\end{document}
